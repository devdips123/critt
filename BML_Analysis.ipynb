{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'kent' from '/data/critt/shared/Spring19/kent.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/data/critt/shared/Spring19/')\n",
    "\n",
    "import kent\n",
    "import importlib\n",
    "importlib.reload(kent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the BML12 data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kent.readTPDDBtables(['BML12/Tables/'], \"*st\", path='/data/critt/tprdb/TPRDB/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Spanish and English Lev_1 dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"spanish_lev1_dic.json\",\"r\").read()\n",
    "spanish_lev1_dic = json.loads(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/users/kent/dsahoo/stoken_lev1_wordlist_new.json\",\"r\").read()\n",
    "english_lev1_dic = json.loads(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: 433, English: 413\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spanish: {len(spanish_lev1_dic)}, English: {len(english_lev1_dic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Spanish and English Frequency dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dic = open('english_freq.json','r').read()\n",
    "english_freq_dic = json.loads(english_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_dic = open('spanish_freq.json','r').read()\n",
    "spanish_freq_dic = json.loads(spanish_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Freq dic: 604205, Spanish Freq dic: 3225296\n"
     ]
    }
   ],
   "source": [
    "print(f\"English Freq dic: {len(english_freq_dic)}, Spanish Freq dic: {len(spanish_freq_dic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-32f61569d02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_to_semantic_words_dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_semantic_similar_word_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m     22\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mgets\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msemantic\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mwords\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mstoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Returns a list of semantic competitor words based on the score_limit\n",
    "def semantic_sim_set(comp_tuple_list, score_limit = 0.7, include_stopwords = False):\n",
    "    comp_list = []\n",
    "    \n",
    "    comp_list = [(word,score) for word,score in comp_tuple_list if word.isalpha() and score >= score_limit ]\n",
    "    return comp_list\n",
    "\n",
    "\n",
    "# Creates a dictionary of word to comp_list for each word in the source_words list\n",
    "def semantic_similar_words_dic(stoken_list, model, topn=100, score_limit=0.7):\n",
    "    token_to_semantic_words_dic = dict()\n",
    "    for stoken in set(stoken_list):\n",
    "        token_to_semantic_words_dic[stoken] = get_semantic_similar_word_list(stoken, model)\n",
    "        \n",
    "    return token_to_semantic_words_dic\n",
    "\n",
    "def get_semantic_similar_word_list(stoken, model=glove_model, topn=100, score_limit=0.7):\n",
    "    \"\"\"\n",
    "    This function gets the semantic similar words of the input stoken\n",
    "    Input\n",
    "        stoken - source token from tpr db\n",
    "        model - word2vec model\n",
    "        topn - maximum number of words to find from the model ( default = 100)\n",
    "        score_limit - threshold similarity score(default = 0.7)\n",
    "    \"\"\"\n",
    "    # Throws KeyError if word not in vocabulary\n",
    "    try:\n",
    "        result = model.most_similar(stoken.lower(), negative=None, topn=topn)\n",
    "    except KeyError:\n",
    "        result = []\n",
    "    comp_list = semantic_sim_set(result,score_limit=score_limit)\n",
    "    \n",
    "    return comp_list\n",
    "\n",
    "## To be used by function summation_freq_simscore()\n",
    "def eval_freq_score(word, score, log_scale, min_freq=20, freq_dic=bnc_freq_dic):\n",
    "    freq = bnc_freq_dic.get(word)\n",
    "    if not freq:\n",
    "        freq = min_freq\n",
    "    if log_scale:\n",
    "        return np.log2(freq)*score\n",
    "    else:\n",
    "        return freq*score\n",
    "    \n",
    "# Calculates entropy given an input of list containing frequency\n",
    "def calc_entropy(freq_list):\n",
    "    entropy = 0.0\n",
    "    if freq_list: \n",
    "        total_freq = sum(freq_list)\n",
    "        prob_list = [f/total_freq for f in freq_list]\n",
    "        entropy = sum([-p*np.log(p) for p in prob_list])\n",
    "    return entropy\n",
    "\n",
    "## Below functions can be applied to a column using .apply() function\n",
    "    \n",
    "def summation_freq_simscore(stoken, freq_dic=bnc_freq_dic, model=glove_model, min_freq=20,log_scale=True):\n",
    "    score = 0.0\n",
    "    sim_list = get_semantic_similar_word_list(stoken, model)\n",
    "    if not sim_list:\n",
    "        return score\n",
    "    #summ_list = [score*freq_dic.get(word) if freq_dic.get(word)  else min_freq*score for word,score in sim_list]\n",
    "    summ_list = [eval_freq_score(word,score,log_scale) for word,score in sim_list]\n",
    "        \n",
    "    return sum(summ_list)\n",
    "\n",
    "def sum_sim_scores(stoken, model=glove_model):\n",
    "    score_sum = 0.0\n",
    "    sim_list = get_semantic_similar_word_list(stoken, model)\n",
    "    if sim_list:\n",
    "        score_list = [score for word,score in sim_list]\n",
    "        score_sum = sum(score_list)\n",
    "    return(score_sum)\n",
    "\n",
    "\n",
    "def entropy_semantic_sim(stoken, model=glove_model, freq_dic=bnc_freq_dic, min_freq=20):\n",
    "    entropy = 0.0\n",
    "    sim_list = get_semantic_similar_word_list(stoken, model)\n",
    "    if sim_list:\n",
    "        freq_list = [ freq_dic.get(word) if freq_dic.get(word) else min_freq for word,freq in sim_list ]\n",
    "        entropy = calc_entropy(freq_list) \n",
    "    \n",
    "    return entropy\n",
    "    \n",
    "def entropy_ortho_sim(stoken, lev_dic=word_to_lev_dic1, freq_dic=bnc_freq_dic, min_freq=20, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of the orthographic similar words(SS) of an input stoken taking into consideration\n",
    "    the frequency of the SS in the freq_dic\n",
    "    Input\n",
    "        stoken - source token from TPR db\n",
    "        lev_dic - dictionary with levenshtein's distance=1\n",
    "        freq_dic - dictionary containing words with its frequency\n",
    "        min_freq - min freq of the word (default=20)\n",
    "    \"\"\"\n",
    "    entropy = 0.0\n",
    "    \n",
    "    sim_words = lev_dic.get(stoken)\n",
    "    if sim_words: \n",
    "        if verbose:\n",
    "            print(f\"Similar words:\\n\\n{sim_words}\")\n",
    "        freq_list = [ freq_dic.get(word) if freq_dic.get(word) else min_freq for word in sim_words ]\n",
    "        if verbose:\n",
    "            print(f\"Similar words freq:\\n\\n{freq_list}\")\n",
    "        entropy = calc_entropy(freq_list) \n",
    "    return entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tgroup_single = set(df[df.TGroup.str.isalpha()]['TGroup'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for TGroup with only single entries\n",
    "df['Single_TToken'] = df['TGroup'].apply(lambda x: x if x.isalpha() else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_per_million(token, dic=spanish_freq_dic):\n",
    "    tpm = 0.0\n",
    "    dic_len = len(dic)\n",
    "    token_freq = dic.get(token)\n",
    "    if token_freq:\n",
    "        tpm = token_freq/dic_len * 1000000\n",
    "        \n",
    "    return round(tpm,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TTokens_per_mil'] = df['Single_TToken'].apply(tokens_per_million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SToken_per_mil'] = df['SToken'].apply(lambda token: tokens_per_million(token, english_freq_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TTokens_per_mil</th>\n",
       "      <th>ProbT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TTokens_per_mil</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.172786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProbT</th>\n",
       "      <td>0.172786</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 TTokens_per_mil     ProbT\n",
       "TTokens_per_mil         1.000000  0.172786\n",
       "ProbT                   0.172786  1.000000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.TTokens_per_mil != 0)][['TTokens_per_mil','ProbT']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Id</th>\n",
       "      <th>SToken</th>\n",
       "      <th>TGroup</th>\n",
       "      <th>Single_TToken</th>\n",
       "      <th>ProbT</th>\n",
       "      <th>AltT</th>\n",
       "      <th>CountT</th>\n",
       "      <th>TTokens_per_mil</th>\n",
       "      <th>SToken_per_mil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3706</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4809</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6642</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7628</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8877</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10030</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9477.27</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10877</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9477.27</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11162</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12023</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13126</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13980</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15112</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16354</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16603</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>disciplina_académica_relativamente_nueva</td>\n",
       "      <td></td>\n",
       "      <td>0.1034</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17749</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>disciplina_académica_relativamente_nueva</td>\n",
       "      <td></td>\n",
       "      <td>0.1034</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18187</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19347</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20443</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22706</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>académica</td>\n",
       "      <td>académica</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>5614.06</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23692</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9477.27</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25108</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>disciplina</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9477.27</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25247</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>academic</td>\n",
       "      <td>disciplina_académica_relativamente_nueva</td>\n",
       "      <td></td>\n",
       "      <td>0.1034</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7926.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Text  Id    SToken                                    TGroup  \\\n",
       "5         5   6  academic                                 académica   \n",
       "1165      5   6  academic                                 académica   \n",
       "2268      5   6  academic                                 académica   \n",
       "2546      5   6  academic                                 académica   \n",
       "3706      5   6  academic                                 académica   \n",
       "4809      5   6  academic                                 académica   \n",
       "5087      5   6  academic                                 académica   \n",
       "6642      5   6  academic                                 académica   \n",
       "7489      5   6  academic                                 académica   \n",
       "7628      5   6  academic                                 académica   \n",
       "8877      5   6  academic                                 académica   \n",
       "10030     5   6  academic                                disciplina   \n",
       "10877     5   6  academic                                disciplina   \n",
       "11162     5   6  academic                                 académica   \n",
       "12023     5   6  academic                                 académica   \n",
       "13126     5   6  academic                                 académica   \n",
       "13980     5   6  academic                                 académica   \n",
       "15112     5   6  academic                                 académica   \n",
       "16354     5   6  academic                                 académica   \n",
       "16603     5   6  academic  disciplina_académica_relativamente_nueva   \n",
       "17749     5   6  academic  disciplina_académica_relativamente_nueva   \n",
       "18187     5   6  academic                                 académica   \n",
       "19347     5   6  academic                                 académica   \n",
       "20443     5   6  academic                                 académica   \n",
       "21603     5   6  academic                                 académica   \n",
       "22706     5   6  academic                                 académica   \n",
       "23692     5   6  academic                                disciplina   \n",
       "25108     5   6  academic                                disciplina   \n",
       "25247     5   6  academic  disciplina_académica_relativamente_nueva   \n",
       "\n",
       "      Single_TToken   ProbT  AltT  CountT  TTokens_per_mil  SToken_per_mil  \n",
       "5         académica  0.7586     3      22          5614.06         7926.12  \n",
       "1165      académica  0.7586     3      22          5614.06         7926.12  \n",
       "2268      académica  0.7586     3      22          5614.06         7926.12  \n",
       "2546      académica  0.7586     3      22          5614.06         7926.12  \n",
       "3706      académica  0.7586     3      22          5614.06         7926.12  \n",
       "4809      académica  0.7586     3      22          5614.06         7926.12  \n",
       "5087      académica  0.7586     3      22          5614.06         7926.12  \n",
       "6642      académica  0.7586     3      22          5614.06         7926.12  \n",
       "7489      académica  0.7586     3      22          5614.06         7926.12  \n",
       "7628      académica  0.7586     3      22          5614.06         7926.12  \n",
       "8877      académica  0.7586     3      22          5614.06         7926.12  \n",
       "10030    disciplina  0.1379     3       4          9477.27         7926.12  \n",
       "10877    disciplina  0.1379     3       4          9477.27         7926.12  \n",
       "11162     académica  0.7586     3      22          5614.06         7926.12  \n",
       "12023     académica  0.7586     3      22          5614.06         7926.12  \n",
       "13126     académica  0.7586     3      22          5614.06         7926.12  \n",
       "13980     académica  0.7586     3      22          5614.06         7926.12  \n",
       "15112     académica  0.7586     3      22          5614.06         7926.12  \n",
       "16354     académica  0.7586     3      22          5614.06         7926.12  \n",
       "16603                0.1034     3       3             0.00         7926.12  \n",
       "17749                0.1034     3       3             0.00         7926.12  \n",
       "18187     académica  0.7586     3      22          5614.06         7926.12  \n",
       "19347     académica  0.7586     3      22          5614.06         7926.12  \n",
       "20443     académica  0.7586     3      22          5614.06         7926.12  \n",
       "21603     académica  0.7586     3      22          5614.06         7926.12  \n",
       "22706     académica  0.7586     3      22          5614.06         7926.12  \n",
       "23692    disciplina  0.1379     3       4          9477.27         7926.12  \n",
       "25108    disciplina  0.1379     3       4          9477.27         7926.12  \n",
       "25247                0.1034     3       3             0.00         7926.12  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.SToken == 'academic')][['Text','Id','SToken','TGroup','Single_TToken','ProbT','AltT','CountT','TTokens_per_mil','SToken_per_mil']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthoSimCount(token,dic):\n",
    "    count = 0\n",
    "    dic_count = dic.get(token)\n",
    "    if dic_count:\n",
    "        count = len(dic_count)\n",
    "        \n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spanish_orthoSim'] = df['SToken'].apply(lambda x: orthoSimCount(x, spanish_lev1_dic))\n",
    "df['english_orthoSim'] = df['SToken'].apply(lambda x: orthoSimCount(x, english_lev1_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['orthoSim'] = df['spanish_orthoSim'] + df['english_orthoSim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STlen'] = df['SToken'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dur</th>\n",
       "      <th>spanish_orthoSim</th>\n",
       "      <th>english_orthoSim</th>\n",
       "      <th>orthoSim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dur</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.007892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish_orthoSim</th>\n",
       "      <td>-0.000380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.719418</td>\n",
       "      <td>0.955709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english_orthoSim</th>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.719418</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orthoSim</th>\n",
       "      <td>0.007892</td>\n",
       "      <td>0.955709</td>\n",
       "      <td>0.891978</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Dur  spanish_orthoSim  english_orthoSim  orthoSim\n",
       "Dur               1.000000         -0.000380          0.019208  0.007892\n",
       "spanish_orthoSim -0.000380          1.000000          0.719418  0.955709\n",
       "english_orthoSim  0.019208          0.719418          1.000000  0.891978\n",
       "orthoSim          0.007892          0.955709          0.891978  1.000000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.orthoSim > 0) & (df.Dur > 0) & (df.STlen > 4)][['Dur','spanish_orthoSim','english_orthoSim','orthoSim']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
