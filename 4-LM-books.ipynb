{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram language models and perplexity\n",
    "1. import texts from NLTK books\n",
    "2. generate and count n-grams\n",
    "3. estimate n-gram probabilities\n",
    "4. compute perplexity of test sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# import nltk resources for lemmatizer evaluation\n",
    "from nltk.book import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence segment and tokenize nltk books and \n",
    "def NLTKbooks2Sent(articles):\n",
    "    \"\"\"\n",
    "    e.g. NLTKbooks2Sent([nltk.book.text1])\n",
    "    param -> Array of type nltk.Text\n",
    "    \n",
    "    returns -> list of list. A list of sentences (Each sentence is a list of tokenized words)\n",
    "               e.g. For input = \"My name is Debasish\" it returns [['My', 'name', 'is', 'Debasish', 'Sahoo']]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for article in articles: \n",
    "        #print(type(article))\n",
    "        for sent in sent_tokenize(' '.join(article)): \n",
    "            data.append(word_tokenize(sent))\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "books1 = NLTKbooks2Sent([nltk.book.text1])\n",
    "books1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute n-grams dictionary for source text\n",
    "import numpy as np\n",
    "\n",
    "N = 3        # length of n-gram\n",
    "\n",
    "# data: list of sentences \n",
    "def nGramCount3(data):\n",
    "    gramsC = {}  # auxilliary dictionary to store n-gram counts\n",
    "    # seg = 1 sentence\n",
    "    for seg in data:\n",
    "        \n",
    "        itm = seg.copy()\n",
    "            \n",
    "        # insert sentence starting and sentence ending symbols\n",
    "        # !!! adjust starting symbol for different length of n-grams N\n",
    "        itm.insert(0, \"///\")\n",
    "        itm.insert(0, \"///\")\n",
    "        itm.append(\"///\")\n",
    "        #print(itm)\n",
    "        # p(a | b ) == count(ab) / count(b)  \n",
    "        for i in range(len(itm)-N+1):\n",
    "            # produce ngram (b)\n",
    "            b  = ' '.join(itm[i:i+N-1]).lower()\n",
    "            # produce an ngram (ab)\n",
    "            ab = ' '.join(itm[i:i+N]).lower()\n",
    "            # print(\"t:{}\\tg:{}\".format(t,g))\n",
    "            gramsC.setdefault(b, {})\n",
    "            gramsC[b].setdefault(ab, 0)\n",
    "            # count the ngram\n",
    "            gramsC[b][ab] += 1\n",
    "    return(gramsC)\n",
    "\n",
    "\n",
    "# compute probability of n-grams\n",
    "# p(a | b ) == count(ab) / count(b) == count(g) / count(t)\n",
    "# loop over all words\n",
    "def nGramProbs3(gramsC) :\n",
    "    nGrams = {}  # final n-gram dictionary with log-prob entries\n",
    "    nMin = 0\n",
    "    for b in gramsC:\n",
    "        # v = number of n-grams that start with b\n",
    "        v = float(sum(gramsC[b].values()))\n",
    "        for ab in gramsC[b]:\n",
    "            # np.log2(v/gramG[b][ab]) == - np.log2(gramG[b][ab]/v)\n",
    "            nGrams[ab] = np.log2(v/gramsC[b][ab])\n",
    "            if(nGrams[ab] > nMin): nMin = nGrams[ab]\n",
    "#            print(\"ab:{:<20} count:{:<6}\\tb:{:<10}  count:{}\\tlog(1/p):{:<5.4}\".format(ab, gramsC[b][ab], b, v,  nGrams[ab]))\n",
    "    nGrams[\"|||OOV|||\"] = nMin+1\n",
    "    return(nGrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"My name is Debasish Sahoo\"\n",
    "my_sent = \"My name is Debasish Sahoo. I study in Kent State University. I stay in mayfield heights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: My name is Debasish Sahoo . I study...>\n",
      "[['My', 'name', 'is', 'Debasish', 'Sahoo', '.'], ['I', 'study', 'in', 'Kent', 'State', 'University', '.'], ['I', 'stay', 'in', 'mayfield', 'heights']]\n",
      "{'/// ///': {'/// /// my': 1, '/// /// i': 2}, '/// my': {'/// my name': 1}, 'my name': {'my name is': 1}, 'name is': {'name is debasish': 1}, 'is debasish': {'is debasish sahoo': 1}, 'debasish sahoo': {'debasish sahoo .': 1}, 'sahoo .': {'sahoo . ///': 1}, '/// i': {'/// i study': 1, '/// i stay': 1}, 'i study': {'i study in': 1}, 'study in': {'study in kent': 1}, 'in kent': {'in kent state': 1}, 'kent state': {'kent state university': 1}, 'state university': {'state university .': 1}, 'university .': {'university . ///': 1}, 'i stay': {'i stay in': 1}, 'stay in': {'stay in mayfield': 1}, 'in mayfield': {'in mayfield heights': 1}, 'mayfield heights': {'mayfield heights ///': 1}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/// /// my': 1.584962500721156,\n",
       " '/// /// i': 0.5849625007211562,\n",
       " '/// my name': 0.0,\n",
       " 'my name is': 0.0,\n",
       " 'name is debasish': 0.0,\n",
       " 'is debasish sahoo': 0.0,\n",
       " 'debasish sahoo .': 0.0,\n",
       " 'sahoo . ///': 0.0,\n",
       " '/// i study': 1.0,\n",
       " '/// i stay': 1.0,\n",
       " 'i study in': 0.0,\n",
       " 'study in kent': 0.0,\n",
       " 'in kent state': 0.0,\n",
       " 'kent state university': 0.0,\n",
       " 'state university .': 0.0,\n",
       " 'university . ///': 0.0,\n",
       " 'i stay in': 0.0,\n",
       " 'stay in mayfield': 0.0,\n",
       " 'in mayfield heights': 0.0,\n",
       " 'mayfield heights ///': 0.0,\n",
       " '|||OOV|||': 2.584962500721156}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to convert normal text to NTLK text\n",
    "my_text_tokens = word_tokenize(my_sent)\n",
    "my_nltk_text = nltk.Text(my_text_tokens)\n",
    "print(my_nltk_text)\n",
    "my_book = NLTKbooks2Sent([my_nltk_text])\n",
    "print(my_book)\n",
    "print(nGramCount3(my_book))\n",
    "nGramProbs3(nGramCount3(my_book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokens = []\n",
    "sent_tokens = sent_tokenize(my_sent)\n",
    "for sent in sent_tokens:\n",
    "    word_tokens.append(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joined_sent = ' '.join(sent_tokens)\n",
    "joined_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute n-grams dictionary for source text\n",
    "import numpy as np\n",
    "\n",
    "N = 2        # length of n-gram\n",
    "\n",
    "# data: list of sentences \n",
    "def nGramCount2(data):\n",
    "    gramsC = {}  # auxilliary dictionary to store n-gram counts\n",
    "    for seg in data:\n",
    "        \n",
    "        itm = seg.copy()\n",
    "            \n",
    "        # insert sentence starting and sentence ending symbols\n",
    "        # !!! adjust starting symbol for different length of n-grams N\n",
    "        #itm.insert(0, \"///\")\n",
    "        itm.insert(0, \"///\")\n",
    "        itm.append(\"///\")\n",
    "        #print(itm)\n",
    "        #P(x1, x2, ..., xn) = P(x1)P(x2|x1)...P(xn|x1,...xn-1)\n",
    "        # p(a | b ) == count(ab) / count(b)  \n",
    "        for i in range(len(itm)-N+1):\n",
    "            # produce ngram (b)\n",
    "            b  = ' '.join(itm[i:i+N-1]).lower()\n",
    "            # produce an ngram (ab)\n",
    "            ab = ' '.join(itm[i:i+N]).lower()\n",
    "            # print(\"t:{}\\tg:{}\".format(t,g))\n",
    "            gramsC.setdefault(b, {})\n",
    "            gramsC[b].setdefault(ab, 0)\n",
    "            # count the ngram\n",
    "            gramsC[b][ab] += 1\n",
    "    return(gramsC)\n",
    "\n",
    "\n",
    "# compute probability of n-grams\n",
    "# p(a | b ) == count(ab) / count(b) == count(g) / count(t)\n",
    "# loop over all words\n",
    "def nGramProbs2(gramsC) :\n",
    "    nGrams = {}  # final n-gram dictionary with log-prob entries\n",
    "    nMin = 0\n",
    "    for b in gramsC:\n",
    "        # v = number of n-grams that start with b\n",
    "        v = float(sum(gramsC[b].values()))\n",
    "        for ab in gramsC[b]:\n",
    "            # np.log2(v/gramG[b][ab]) == - np.log2(gramG[b][ab]/v)\n",
    "            nGrams[ab] = np.log2(v/gramsC[b][ab])\n",
    "            if(nGrams[ab] > nMin): nMin = nGrams[ab]\n",
    "#            print(\"ab:{:<20} count:{:<6}\\tb:{:<10}  count:{}\\tlog(1/p):{:<5.4}\".format(ab, gramsC[b][ab], b, v,  nGrams[ab]))\n",
    "    nGrams[\"|||OOV|||\"] = nMin+1\n",
    "    return(nGrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute n-grams dictionary for source text\n",
    "import numpy as np\n",
    "\n",
    "N = 1        # length of n-gram\n",
    "\n",
    "# data: list of sentences \n",
    "def nGramCount1(data):\n",
    "    gramsC = {}  # auxilliary dictionary to store n-gram counts\n",
    "    for seg in data:\n",
    "        \n",
    "        itm = seg.copy()\n",
    "            \n",
    "        # insert sentence starting and sentence ending symbols\n",
    "        # !!! adjust starting symbol for different length of n-grams N\n",
    "        #itm.insert(0, \"///\")\n",
    "        #itm.insert(0, \"///\")\n",
    "        #itm.append(\"///\")\n",
    "        #print(itm)\n",
    "        #P(x1, x2, ..., xn) = P(x1)P(x2|x1)...P(xn|x1,...xn-1)\n",
    "        # p(a | b ) == count(ab) / count(b)  \n",
    "        for i in range(len(itm)-N+1):\n",
    "            # produce ngram (b)\n",
    "            b  = ' '.join(itm[i:i+N-1]).lower()\n",
    "            # produce an ngram (ab)\n",
    "            ab = ' '.join(itm[i:i+N]).lower()\n",
    "            # print(\"t:{}\\tg:{}\".format(t,g))\n",
    "            gramsC.setdefault(b, {})\n",
    "            gramsC[b].setdefault(ab, 0)\n",
    "            # count the ngram\n",
    "            gramsC[b][ab] += 1\n",
    "    return(gramsC)\n",
    "\n",
    "\n",
    "# compute probability of n-grams\n",
    "# p(a | b ) == count(ab) / count(b) == count(g) / count(t)\n",
    "# loop over all words\n",
    "def nGramProbs1(gramsC) :\n",
    "    nGrams = {}  # final n-gram dictionary with log-prob entries\n",
    "    nMin = 0\n",
    "    for b in gramsC:\n",
    "        # v = number of n-grams that start with b\n",
    "        v = float(sum(gramsC[b].values()))\n",
    "        for ab in gramsC[b]:\n",
    "            # np.log2(v/gramG[b][ab]) == - np.log2(gramG[b][ab]/v)\n",
    "            nGrams[ab] = np.log2(v/gramsC[b][ab])\n",
    "            if(nGrams[ab] > nMin): nMin = nGrams[ab]\n",
    "#            print(\"ab:{:<20} count:{:<6}\\tb:{:<10}  count:{}\\tlog(1/p):{:<5.4}\".format(ab, gramsC[b][ab], b, v,  nGrams[ab]))\n",
    "    nGrams[\"|||OOV|||\"] = nMin+1\n",
    "    return(nGrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity of segments\n",
    "\n",
    "def perplexity2(data, nGrams):\n",
    "    N = 2\n",
    "    oov = 0\n",
    "    PP = []\n",
    "    for seg in data:  \n",
    "        itm = seg.copy()\n",
    "        \n",
    "        # !!! adjust starting symbol for different length of n-grams N\n",
    "        itm.insert(0, \"///\")\n",
    "        #itm.insert(0, \"///\")\n",
    "        itm.append(\"///\")\n",
    "\n",
    "        H = 0\n",
    "        for i in range(len(itm)-N):\n",
    "            ab = ' '.join(itm[i:i+N]).lower()\n",
    "            try: \n",
    "                H += nGrams[ab]\n",
    "                print(\"nGram: {:20}\\t{:4.4}\".format(ab, nGrams[ab]))\n",
    "            except:\n",
    "                print(\"nGram: {:20}\\t{:4.4}\\tundef\".format(ab, nGrams[\"|||OOV|||\"]))\n",
    "                H += nGrams[\"|||OOV|||\"]\n",
    "                oov += 1\n",
    "        p = (2**H)**(1 / float(len(seg)))\n",
    "        PP.append(p)\n",
    "        print(\"PP:{:5.2f}\\tlen:{}\\tseg:{}\".format(p, len(seg), seg))\n",
    "    print(\"Number of OOV 2-grams: \"+ str(oov))\n",
    "    return(PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity of segments\n",
    "\n",
    "def perplexity1(data, nGrams):\n",
    "    N = 1\n",
    "    oov = 0\n",
    "    PP = []\n",
    "    for seg in data:  \n",
    "        itm = seg.copy()\n",
    "        \n",
    "        # !!! adjust starting symbol for different length of n-grams N\n",
    "        #itm.insert(0, \"///\")\n",
    "        #itm.insert(0, \"///\")\n",
    "        #itm.append(\"///\")\n",
    "\n",
    "        H = 0\n",
    "        for i in range(len(itm)-N):\n",
    "            ab = ' '.join(itm[i:i+N]).lower()\n",
    "            try: \n",
    "                H += nGrams[ab]\n",
    "                print(\"nGram: {:20}\\t{:4.4}\".format(ab, nGrams[ab]))\n",
    "            except:\n",
    "                print(\"nGram: {:20}\\t{:4.4}\\tundef\".format(ab, nGrams[\"|||OOV|||\"]))\n",
    "                H += nGrams[\"|||OOV|||\"]\n",
    "                oov++\n",
    "        p = (2**H)**(1 / float(len(seg)))\n",
    "        PP.append(p)\n",
    "        print(\"PP:{:5.2f}\\tlen:{}\\tseg:{}\".format(p, len(seg), seg))\n",
    "    print(\"Number of OOV 3-grams: \"+ str(oov))\n",
    "    return(PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity of segments\n",
    "\n",
    "def perplexity(data, nGrams):\n",
    "    N = 3\n",
    "    PP = []\n",
    "    oov = 0\n",
    "    for seg in data:  \n",
    "        itm = seg.copy()\n",
    "        \n",
    "        # !!! adjust starting symbol for different length of n-grams N\n",
    "        itm.insert(0, \"///\")\n",
    "        itm.insert(0, \"///\")\n",
    "        itm.append(\"///\")\n",
    "\n",
    "        H = 0\n",
    "        for i in range(len(itm)-N):\n",
    "            ab = ' '.join(itm[i:i+N]).lower()\n",
    "            try: \n",
    "                H += nGrams[ab]\n",
    "                print(\"nGram: {:20}\\t{:4.4}\".format(ab, nGrams[ab]))\n",
    "            except:\n",
    "                print(\"nGram: {:20}\\t{:4.4}\\tundef\".format(ab, nGrams[\"|||OOV|||\"]))\n",
    "                H += nGrams[\"|||OOV|||\"]\n",
    "                oov += 1\n",
    "        p = (2**H)**(1 / float(len(seg)))\n",
    "        PP.append(p)\n",
    "        print(\"PP:{:5.2f}\\tlen:{}\\tseg:{}\".format(p, len(seg), seg))\n",
    "    print(\"Number of OOV 3-grams: \"+ str(oov))\n",
    "    return(PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "# prepare different sets of books \n",
    "books18 = NLTKbooks2Sent([nltk.book.text1, nltk.book.text2, nltk.book.text3,nltk.book.text4, nltk.book.text6, nltk.book.text7,nltk.book.text8])\n",
    "books1 = NLTKbooks2Sent([nltk.book.text1])\n",
    "books9 = NLTKbooks2Sent([nltk.book.text9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n-gram language models\n",
    "NG18 = nGramProbs3(nGramCount3(books18))\n",
    "NG1 = nGramProbs3(nGramCount3(books1))\n",
    "NG9 = nGramProbs3(nGramCount3(books9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "print(N)\n",
    "#len(reuters.sents())\n",
    "#print(reuters.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuter = nGramProbs(nGramCount(reuters.sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nGram: ///                 \t3.289\n",
      "nGram: this                \t7.717\n",
      "nGram: is                  \t7.41\n",
      "nGram: a                   \t5.953\n",
      "nGram: test                \t16.16\n",
      "PP:1122.68\tlen:4\tseg:['This', 'is', 'a', 'test']\n",
      "nGram: ///                 \t3.289\n",
      "nGram: quite               \t12.8\n",
      "nGram: unlikely            \t18.16\n",
      "nGram: test                \t16.16\n",
      "PP:114578.63\tlen:3\tseg:['quite', 'unlikely', 'test']\n",
      "nGram: ///                 \t3.255\n",
      "nGram: this                \t8.03\n",
      "nGram: is                  \t7.339\n",
      "nGram: a                   \t6.11\n",
      "nGram: test                \t13.71\n",
      "PP:782.31\tlen:4\tseg:['This', 'is', 'a', 'test']\n",
      "nGram: ///                 \t3.255\n",
      "nGram: quite               \t12.38\n",
      "nGram: unlikely            \t17.03\n",
      "nGram: test                \t13.71\n",
      "PP:45090.29\tlen:3\tseg:['quite', 'unlikely', 'test']\n",
      "nGram: ///                 \t3.522\n",
      "nGram: this                \t9.066\n",
      "nGram: is                  \t7.942\n",
      "nGram: a                   \t6.231\n",
      "nGram: test                \t14.31\n",
      "PP:1232.01\tlen:4\tseg:['This', 'is', 'a', 'test']\n",
      "nGram: ///                 \t3.522\n",
      "nGram: quite               \t15.26\n",
      "nGram: unlikely            \t13.5\n",
      "nGram: test                \t14.31\n",
      "PP:47284.56\tlen:3\tseg:['quite', 'unlikely', 'test']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1232.009700152163, 47284.55555491499]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity([[\"This\", \"is\", \"a\", \"test\"],[\"quite\",\"unlikely\",\"test\"]], NG1)\n",
    "perplexity([[\"This\", \"is\", \"a\", \"test\"],[\"quite\",\"unlikely\",\"test\"]], NG18)\n",
    "perplexity([[\"This\", \"is\", \"a\", \"test\"],[\"quite\",\"unlikely\",\"test\"]], reuter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test language model on training test\n",
    "perplexity(books9[:1], NG1)\n",
    "perplexity(books9[:1], NG9)\n",
    "perplexity(books9[:1], NG18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks:\n",
    "#  add more data to the LM and assess impact on perplexity\n",
    "#  generate language models with KENTstemmer/porter/snowball/wn stemmer\n",
    "#  think of methods to reduce number of OOV words (numbers, proper names, etc)\n",
    "#  compare 1-gram language model\n",
    "#  compare 3-gram language model\n",
    "#\n",
    "#  Scramble (paraphrase) words of a sentence and compare perplexity with n-gram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import KENTstemmer3\n",
    "import importlib\n",
    "import nltk\n",
    "\n",
    "importlib.reload(KENTstemmer3)\n",
    "#print(my_book)\n",
    "\n",
    "my_stemmed_book = []\n",
    "dic = KENTstemmer3.readDictionary(\"tokenLemma.txt\")\n",
    "\n",
    "#tags = nltk.pos_tag(['accompanied','unlikely', 'lovable'])\n",
    "for sent in books1:\n",
    "    tags = nltk.pos_tag(sent)\n",
    "    #print(tags)\n",
    "    stems = []\n",
    "    for tag in tags:\n",
    "        stems.append(KENTstemmer3.KENTstemmer3(tag, dic))\n",
    "    #print(stems)\n",
    "    my_stemmed_book.append(stems)\n",
    "print(my_stemmed_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import KENTstemmer3\n",
    "import importlib\n",
    "import nltk\n",
    "\n",
    "importlib.reload(KENTstemmer3)\n",
    "dic = KENTstemmer3.readDictionary(\"tokenLemma.txt\")\n",
    "\n",
    "def apply_kent_stemmer(book) :\n",
    "    \"\"\"\n",
    "    params \n",
    "        book -> type nltk.Text for e.g. nltk.book.text1\n",
    "             -> type a list of list for e.g. [ word_tokenize('This is a test') ]\n",
    "                                             [word_tokenize(\"i am a good boy.\"), word_tokenize('how are you')]\n",
    "             \n",
    "    returns\n",
    "        stemmed_book -> a list of list containing stemmed words of the params\n",
    "    \n",
    "    \"\"\"\n",
    "    my_stemmed_book = []\n",
    "    for sent in book:\n",
    "        tags = nltk.pos_tag(sent)\n",
    "        #print(tags)\n",
    "        stems = []\n",
    "        for tag in tags:\n",
    "            stems.append(KENTstemmer3.KENTstemmer3(tag, dic))\n",
    "        #print(stems)\n",
    "        my_stemmed_book.append(stems)\n",
    "    #print(my_stemmed_book)\n",
    "    return my_stemmed_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_books1 = apply_kent_stemmer(books1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nGram: /// /// we          \t7.765\n",
      "nGram: /// we have         \t4.524\n",
      "nGram: we have found       \t14.29\tundef\n",
      "nGram: have found common   \t14.29\tundef\n",
      "nGram: found common things \t14.29\tundef\n",
      "nGram: common things at    \t14.29\tundef\n",
      "nGram: things at last      \t14.29\tundef\n",
      "nGram: at last and         \t14.29\tundef\n",
      "nGram: last and marriage   \t14.29\tundef\n",
      "nGram: and marriage and    \t14.29\tundef\n",
      "nGram: marriage and a      \t14.29\tundef\n",
      "nGram: and a creed         \t14.29\tundef\n",
      "nGram: a creed ,           \t14.29\tundef\n",
      "nGram: creed , and         \t14.29\tundef\n",
      "nGram: , and i             \t5.975\n",
      "nGram: and i may           \t6.476\n",
      "nGram: i may safely        \t14.29\tundef\n",
      "nGram: may safely write    \t14.29\tundef\n",
      "nGram: safely write it     \t14.29\tundef\n",
      "nGram: write it now        \t14.29\tundef\n",
      "nGram: it now ,            \t2.807\n",
      "nGram: now , and           \t4.285\n",
      "nGram: , and you           \t8.367\n",
      "nGram: and you may         \t3.807\n",
      "nGram: you may safely      \t14.29\tundef\n",
      "nGram: may safely read     \t14.29\tundef\n",
      "nGram: safely read .       \t14.29\tundef\n",
      "PP:3292.30\tlen:27\tseg:['We', 'have', 'found', 'common', 'things', 'at', 'last', 'and', 'marriage', 'and', 'a', 'creed', ',', 'And', 'I', 'may', 'safely', 'write', 'it', 'now', ',', 'and', 'you', 'may', 'safely', 'read', '.']\n",
      "nGram: /// /// g           \t14.29\tundef\n",
      "nGram: /// g .             \t14.29\tundef\n",
      "PP:20014.00\tlen:2\tseg:['G', '.']\n",
      "nGram: /// /// k           \t14.29\tundef\n",
      "nGram: /// k .             \t14.29\tundef\n",
      "PP:20014.00\tlen:2\tseg:['K', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3292.2950254301463, 20014.0, 20014.0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NG1 = nGramProbs3(nGramCount3(books1))\n",
    "perplexity(books9[18:21], NG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nGram: /// /// we          \t7.561\n",
      "nGram: /// we have         \t3.728\n",
      "nGram: we have found       \t14.29\tundef\n",
      "nGram: have found common   \t14.29\tundef\n",
      "nGram: found common thing  \t14.29\tundef\n",
      "nGram: common thing at     \t14.29\tundef\n",
      "nGram: thing at last       \t14.29\tundef\n",
      "nGram: at last and         \t14.29\tundef\n",
      "nGram: last and marriage   \t14.29\tundef\n",
      "nGram: and marriage and    \t14.29\tundef\n",
      "nGram: marriage and a      \t14.29\tundef\n",
      "nGram: and a cre           \t14.29\tundef\n",
      "nGram: a cre ,             \t14.29\tundef\n",
      "nGram: cre , and           \t1.585\n",
      "nGram: , and i             \t5.875\n",
      "nGram: and i may           \t 6.7\n",
      "nGram: i may safe          \t14.29\tundef\n",
      "nGram: may safe write      \t14.29\tundef\n",
      "nGram: safe write it       \t14.29\tundef\n",
      "nGram: write it now        \t14.29\tundef\n",
      "nGram: it now ,            \t 3.0\n",
      "nGram: now , and           \t4.285\n",
      "nGram: , and you           \t7.666\n",
      "nGram: and you may         \t4.807\n",
      "nGram: you may safe        \t14.29\tundef\n",
      "nGram: may safe read       \t14.29\tundef\n",
      "nGram: safe read .         \t14.29\tundef\n",
      "PP:2352.86\tlen:27\tseg:['we', 'have', 'found', 'common', 'thing', 'at', 'last', 'and', 'marriage', 'and', 'a', 'cre', ',', 'and', 'i', 'may', 'safe', 'write', 'it', 'now', ',', 'and', 'you', 'may', 'safe', 'read', '.']\n",
      "nGram: /// /// g           \t11.7\n",
      "nGram: /// g .             \t14.29\tundef\n",
      "PP:8170.68\tlen:2\tseg:['g', '.']\n",
      "nGram: /// /// k           \t13.29\n",
      "nGram: /// k .             \t14.29\tundef\n",
      "PP:14152.04\tlen:2\tseg:['k', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2352.85555556207, 8170.6812853437605, 14152.035118667563]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NG1_stemmed = nGramProbs3(nGramCount3(stemmed_books1))\n",
    "perplexity(apply_kent_stemmer(books9[18:21]), NG1_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.24 s, sys: 644 ms, total: 5.89 s\n",
      "Wall time: 5.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bnc = []\n",
    "\n",
    "with open('spring19/lemmaLexicon/BNC.seg', 'r') as f:\n",
    "    for line in f :\n",
    "        bnc.append(line.replace('\\n',''))\n",
    "f.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6052201"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bnc_book_tokens = [word_tokenize(l) for l in bnc[:100000] ]\n",
    "stemmed_bnc_book = apply_kent_stemmer(bnc_book_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 51s, sys: 18.2 s, total: 30min 9s\n",
      "Wall time: 30min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stemmed_bnc_book = []\n",
    "for l in bnc[:1000000]:\n",
    "    aaa = apply_kent_stemmer([word_tokenize(l)])\n",
    "    stemmed_bnc_book.append(aaa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stemmed_bnc_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NG_train_bnc = nGramProbs3(nGramCount3(stemmed_bnc_book))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NG_train_bnc2 = nGramProbs2(nGramCount2(stemmed_bnc_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "NG_train_bnc1 = nGramProbs1(nGramCount1(stemmed_bnc_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/// /// factsheet': 16.609640474436812,\n",
       " '/// /// aids': 12.802285552379209,\n",
       " '/// /// this': 5.5175440594460206,\n",
       " '/// /// how': 8.434714791936132,\n",
       " '/// /// through': 12.522177633186473,\n",
       " '/// /// from': 8.888541285729627,\n",
       " '/// /// it': 4.703877424834019,\n",
       " '/// /// the': 3.194691032962548,\n",
       " '/// /// often': 11.702749878828293,\n",
       " '/// /// do': 8.419815915556795,\n",
       " '/// /// there': 6.23026210736555,\n",
       " '/// /// 10': 12.90920075629572,\n",
       " '/// /// you': 6.839802630807371,\n",
       " '/// /// 7': 12.287712379549449,\n",
       " '/// /// woman': 12.150208855799514,\n",
       " '/// /// in': 5.197070627631602,\n",
       " '/// /// 1': 9.789461512021624,\n",
       " '/// /// &': 2.970412834089892,\n",
       " '/// /// useful': 13.802285552379209,\n",
       " '/// /// rais': 15.609640474436812,\n",
       " '/// /// whether': 10.965784284662087,\n",
       " '/// /// below': 12.90920075629572,\n",
       " '/// /// none': 11.75165947930924,\n",
       " '/// /// car': 13.287712379549449,\n",
       " '/// /// i': 5.031739637549842,\n",
       " '/// /// 2': 10.150208855799514,\n",
       " '/// /// jumble': 16.609640474436812,\n",
       " '/// /// 3': 10.4397154729945,\n",
       " '/// /// sponsor': 14.024677973715656,\n",
       " '/// /// 4': 11.117787378107137,\n",
       " '/// /// have': 9.047398050215739,\n",
       " '/// /// 5': 11.655444164049937,\n",
       " '/// /// a': 5.854752972273343,\n",
       " '/// /// but': 5.223239050796034,\n",
       " '/// /// 6': 11.854752972273344,\n",
       " '/// /// hold': 12.702749878828293,\n",
       " '/// /// 8': 11.75165947930924,\n",
       " '/// /// kiddies': 16.609640474436812,\n",
       " '/// /// organise': 15.024677973715656,\n",
       " '/// /// if': 6.228097523252226,\n",
       " '/// /// acet': 11.400187108807861,\n",
       " '/// /// what': 7.305859726259709,\n",
       " '/// /// transport': 14.287712379549449,\n",
       " '/// /// many': 8.69675113820685,\n",
       " '/// /// yes': 10.678903136873926,\n",
       " '/// /// after': 8.05889368905357,\n",
       " '/// /// we': 6.61246099349919,\n",
       " '/// /// volunteer': 14.024677973715656,\n",
       " '/// /// telephone': 13.150208855799514,\n",
       " '/// /// cliff': 15.609640474436812,\n",
       " '/// /// so': 7.535499011684307,\n",
       " '/// /// home': 13.150208855799514,\n",
       " '/// /// strathclyde': 15.609640474436812,\n",
       " '/// /// margaret': 13.150208855799514,\n",
       " '/// /// dr': 10.234601043089887,\n",
       " '/// /// three-quarter': 16.609640474436812,\n",
       " '/// /// churche': 14.609640474436812,\n",
       " '/// /// those': 9.764150423492437,\n",
       " '/// /// both': 9.287712379549449,\n",
       " '/// /// third': 12.150208855799514,\n",
       " '/// /// official': 11.965784284662087,\n",
       " '/// /// 24,000': 16.609640474436812,\n",
       " '/// /// bulletin': 16.609640474436812,\n",
       " '/// /// nurse': 15.024677973715656,\n",
       " '/// /// purchase': 14.609640474436812,\n",
       " '/// /// last': 9.522177633186473,\n",
       " '/// /// uk': 13.024677973715656,\n",
       " '/// /// interest': 12.90920075629572,\n",
       " '/// /// like': 9.655444164049937,\n",
       " '/// /// letter': 13.4397154729945,\n",
       " '/// /// any': 10.07048166332878,\n",
       " '/// /// please': 10.994930630321603,\n",
       " '/// /// box': 12.702749878828293,\n",
       " '/// /// be': 8.90920075629572,\n",
       " '/// /// over': 10.419815915556795,\n",
       " '/// /// to': 7.770436686339868,\n",
       " '/// /// an': 8.543551283979038,\n",
       " '/// /// peter': 11.024677973715656,\n",
       " '/// /// these': 7.844768883700721,\n",
       " '/// /// return': 13.287712379549449,\n",
       " '/// /// live': 13.024677973715656,\n",
       " '/// /// despite': 9.739275754853407,\n",
       " '/// /// while': 9.002310160687202,\n",
       " '/// /// even': 8.37602079767711,\n",
       " '/// /// help': 12.150208855799514,\n",
       " '/// /// illnesse': 15.609640474436812,\n",
       " '/// /// recent': 11.802285552379209,\n",
       " '/// /// basical': 13.150208855799514,\n",
       " '/// /// organisation': 15.024677973715656,\n",
       " '/// /// without': 10.587272661408358,\n",
       " '/// /// by': 8.296757519152457,\n",
       " '/// /// on': 7.34285393374191,\n",
       " '/// /// pre-record': 16.609640474436812,\n",
       " '/// /// just': 9.764150423492437,\n",
       " '/// /// during': 9.854752972273344,\n",
       " '/// /// two': 9.09394063615277,\n",
       " '/// /// patrick': 12.150208855799514,\n",
       " '/// /// change': 12.90920075629572,\n",
       " '/// /// brian': 12.361712960993227,\n",
       " '/// /// denise': 16.609640474436812,\n",
       " '/// /// maurice': 15.024677973715656,\n",
       " '/// /// rev': 15.609640474436812,\n",
       " '/// /// programme': 15.609640474436812,\n",
       " '/// /// romania': 15.609640474436812,\n",
       " '/// /// kate': 15.024677973715656,\n",
       " '/// /// speak': 12.361712960993227,\n",
       " '/// /// ten': 12.150208855799514,\n",
       " '/// /// new': 10.45989335493213,\n",
       " '/// /// between': 11.609640474436812,\n",
       " '/// /// patient': 14.024677973715656,\n",
       " '/// /// cause': 16.609640474436812,\n",
       " '/// /// with': 8.278723596322195,\n",
       " '/// /// plan': 12.086078518379798,\n",
       " '/// /// slow': 11.117787378107137,\n",
       " '/// /// internal': 14.609640474436812,\n",
       " '/// /// survive': 15.609640474436812,\n",
       " '/// /// as': 6.791058296955954,\n",
       " '/// /// specialist': 13.802285552379209,\n",
       " '/// /// until': 10.828280760912152,\n",
       " '/// /// professionally-bas': 16.609640474436812,\n",
       " '/// /// forward': 14.609640474436812,\n",
       " '/// /// care-link': 16.609640474436812,\n",
       " '/// /// design': 13.024677973715656,\n",
       " '/// /// referral': 15.609640474436812,\n",
       " '/// /// at': 7.19812948642474,\n",
       " '/// /// around': 12.150208855799514,\n",
       " '/// /// for': 6.864806636937266,\n",
       " '/// /// he': 5.045967514609298,\n",
       " '/// /// work': 11.086078518379798,\n",
       " '/// /// kay': 16.609640474436812,\n",
       " '/// /// child': 13.024677973715656,\n",
       " '/// /// some': 7.940755490170565,\n",
       " '/// /// portsmouth': 15.609640474436812,\n",
       " '/// /// (': 7.838151004936213,\n",
       " '/// /// hampshire': 15.609640474436812,\n",
       " '/// /// p.a.l.s': 16.609640474436812,\n",
       " '/// /// provide': 15.024677973715656,\n",
       " '/// /// whilst': 14.024677973715656,\n",
       " '/// /// notice': 13.024677973715656,\n",
       " '/// /// next': 10.75165947930924,\n",
       " '/// /// follow': 11.324238255574564,\n",
       " '/// /// alison': 14.287712379549449,\n",
       " '/// /// t.e.s.p.i': 16.609640474436812,\n",
       " '/// /// catherine': 14.609640474436812,\n",
       " '/// /// midwifery': 16.609640474436812,\n",
       " '/// /// area': 15.609640474436812,\n",
       " '/// /// within': 11.183375719734714,\n",
       " '/// /// likewise': 13.609640474436812,\n",
       " '/// /// tony': 11.965784284662087,\n",
       " '/// /// one': 7.485519162607624,\n",
       " '/// /// 9.32am': 16.609640474436812,\n",
       " '/// /// 10.10am': 16.609640474436812,\n",
       " '/// /// meanwhile': 10.305859726259708,\n",
       " '/// /// westminster': 15.024677973715656,\n",
       " '/// /// no': 7.90920075629572,\n",
       " '/// /// 11.00am': 16.609640474436812,\n",
       " '/// /// 1.04pm': 16.609640474436812,\n",
       " '/// /// 1.05pm': 16.609640474436812,\n",
       " '/// /// 2.30pm': 16.609640474436812,\n",
       " '/// /// although': 8.75165947930924,\n",
       " '/// /// 3.35pm': 16.609640474436812,\n",
       " '/// /// 4.00pm': 16.609640474436812,\n",
       " '/// /// 4.55pm': 16.609640474436812,\n",
       " '/// /// nobody': 11.854752972273344,\n",
       " '/// /// she': 6.133907043470414,\n",
       " '/// /// 5.45pm': 16.609640474436812,\n",
       " '/// /// 6.30pm': 16.609640474436812,\n",
       " '/// /// of': 8.923139947253594,\n",
       " '/// /// all': 8.013450718292402,\n",
       " '/// /// reg': 16.609640474436812,\n",
       " '/// /// 2245302': 16.609640474436812,\n",
       " '/// /// 299293': 16.609640474436812,\n",
       " '/// /// register': 16.609640474436812,\n",
       " '/// /// hiv': 15.609640474436812,\n",
       " '/// /// and': 6.1791879227712805,\n",
       " '/// /// once': 9.305859726259708,\n",
       " '/// /// friend': 12.802285552379209,\n",
       " '/// /// safe': 15.609640474436812,\n",
       " '/// /// risky': 16.609640474436812,\n",
       " '/// /// then': 8.274250119742886,\n",
       " '/// /// they': 6.001385652458903,\n",
       " '/// /// now': 8.57072148514451,\n",
       " '/// /// hospital': 13.802285552379209,\n",
       " '/// /// most': 8.874930854210973,\n",
       " '/// /// ,': 10.039784866105864,\n",
       " '/// /// *': 13.024677973715656,\n",
       " '/// /// b': 12.086078518379798,\n",
       " '/// /// c': 11.965784284662087,\n",
       " '/// /// tom': 12.287712379549449,\n",
       " '/// /// still': 11.055051622759175,\n",
       " '/// /// when': 7.24550581942876,\n",
       " '/// /// sexual': 15.609640474436812,\n",
       " '/// /// tell-tale': 16.609640474436812,\n",
       " '/// /// sex': 14.024677973715656,\n",
       " '/// /// stay': 13.287712379549449,\n",
       " '/// /// also': 9.789461512021624,\n",
       " '/// /// low': 12.024677973715656,\n",
       " '/// /// higher': 16.609640474436812,\n",
       " '/// /// very': 11.324238255574564,\n",
       " '/// /// why': 9.101845834238116,\n",
       " '/// /// stuff': 15.609640474436812,\n",
       " '/// /// particular': 12.802285552379209,\n",
       " '/// /// because': 9.828280760912152,\n",
       " '/// /// ok': 13.150208855799514,\n",
       " '/// /// drug': 13.024677973715656,\n",
       " '/// /// trad': 13.609640474436812,\n",
       " '/// /// steal': 15.024677973715656,\n",
       " '/// /// thing': 12.21732305165805,\n",
       " '/// /// try': 11.055051622759175,\n",
       " '/// /// no-one': 13.802285552379209,\n",
       " '/// /// strong': 13.024677973715656,\n",
       " '/// /// friendship': 15.024677973715656,\n",
       " '/// /// people': 10.07048166332878,\n",
       " '/// /// alway': 11.4397154729945,\n",
       " '/// /// reduce': 14.287712379549449,\n",
       " '/// /// aim': 13.802285552379209,\n",
       " '/// /// see': 11.287712379549449,\n",
       " '/// /// text': 16.609640474436812,\n",
       " '/// /// photograph': 15.024677973715656,\n",
       " '/// /// exhaustion': 15.609640474436812,\n",
       " '/// /// educ': 13.150208855799514,\n",
       " '/// /// overseas': 16.609640474436812,\n",
       " '/// /// thank': 11.522177633186473,\n",
       " '/// /// initial': 12.361712960993227,\n",
       " '/// /// since': 9.191787959550913,\n",
       " '/// /// donation': 15.609640474436812,\n",
       " '/// /// is': 9.714822711128868,\n",
       " '/// /// not': 8.25649364893873,\n",
       " '/// /// make': 11.287712379549449,\n",
       " '/// /// can': 10.75165947930924,\n",
       " '/// /// or': 9.208761038154627,\n",
       " '/// /// again': 10.994930630321603,\n",
       " '/// /// covenant': 14.609640474436812,\n",
       " '/// /// us': 11.655444164049937,\n",
       " '/// /// alternative': 11.655444164049937,\n",
       " '/// /// which': 10.965784284662087,\n",
       " '/// /// either': 11.802285552379209,\n",
       " '/// /// thus': 9.75165947930924,\n",
       " '/// /// however': 7.429731384421878,\n",
       " '/// /// whatever': 11.183375719734714,\n",
       " '/// /// gift': 15.024677973715656,\n",
       " '/// /// provid': 12.4397154729945,\n",
       " '/// /// say': 12.4397154729945,\n",
       " '/// /// cash': 13.4397154729945,\n",
       " '/// /// nor': 10.133907043470414,\n",
       " '/// /// payment': 14.609640474436812,\n",
       " '/// /// month': 14.287712379549449,\n",
       " '/// /// care': 13.802285552379209,\n",
       " '/// /// similar': 10.90920075629572,\n",
       " '/// /// legacy': 15.609640474436812,\n",
       " '/// /// moreover': 10.150208855799514,\n",
       " '/// /// who': 10.252088469818728,\n",
       " '/// /// choose': 12.287712379549449,\n",
       " '/// /// anyth': 13.024677973715656,\n",
       " '/// /// money': 13.150208855799514,\n",
       " '/// /// london': 11.480357457491845,\n",
       " '/// /// nurs': 15.609640474436812,\n",
       " '/// /// practical': 14.287712379549449,\n",
       " '/// /// medical': 13.609640474436812,\n",
       " '/// /// meet': 14.024677973715656,\n",
       " '/// /// already': 11.75165947930924,\n",
       " '/// /// world': 13.150208855799514,\n",
       " '/// /// regional': 15.024677973715656,\n",
       " '/// /// outside': 13.024677973715656,\n",
       " '/// /// prevention': 14.024677973715656,\n",
       " '/// /// survey': 14.024677973715656,\n",
       " '/// /// patron': 16.609640474436812,\n",
       " '/// /// robert': 12.21732305165805,\n",
       " '/// /// governmental': 16.609640474436812,\n",
       " '/// /// every': 10.75165947930924,\n",
       " '/// /// that': 7.422288401236315,\n",
       " '/// /// sir': 10.702749878828293,\n",
       " '/// /// director': 13.802285552379209,\n",
       " '/// /// complex': 15.024677973715656,\n",
       " '/// /// effective': 15.024677973715656,\n",
       " '/// /// evaluation': 16.609640474436812,\n",
       " '/// /// school': 13.802285552379209,\n",
       " '/// /// africa': 15.609640474436812,\n",
       " '/// /// whole': 15.609640474436812,\n",
       " '/// /// half': 13.024677973715656,\n",
       " '/// /// today': 10.361712960993227,\n",
       " '/// /// british': 11.4397154729945,\n",
       " '/// /// instead': 10.133907043470414,\n",
       " '/// /// p.': 14.609640474436812,\n",
       " '/// /// nightsit': 16.609640474436812,\n",
       " '/// /// family': 14.024677973715656,\n",
       " '/// /// man': 12.702749878828293,\n",
       " '/// /// preventing': 16.609640474436812,\n",
       " '/// /// ultimate': 13.609640474436812,\n",
       " '/// /// tragical': 16.609640474436812,\n",
       " '/// /// videos': 16.609640474436812,\n",
       " '/// /// furthermore': 10.965784284662087,\n",
       " '/// /// prejudice': 16.609640474436812,\n",
       " '/// /// pupil': 15.609640474436812,\n",
       " '/// /// hiv/aids': 16.609640474436812,\n",
       " '/// /// seven': 13.802285552379209,\n",
       " '/// /// uganda': 15.609640474436812,\n",
       " '/// /// indicate': 14.609640474436812,\n",
       " '/// /// adult': 15.609640474436812,\n",
       " '/// /// auditors': 16.609640474436812,\n",
       " '/// /// neville': 16.609640474436812,\n",
       " '/// /// amnesty': 12.21732305165805,\n",
       " '/// /// page': 10.702749878828293,\n",
       " '/// /// classifi': 15.609640474436812,\n",
       " '/// /// thoza': 15.609640474436812,\n",
       " '/// /// george': 12.287712379549449,\n",
       " '/// /// oth': 9.764150423492437,\n",
       " '/// /// other': 11.055051622759175,\n",
       " '/// /// swaziland': 16.609640474436812,\n",
       " '/// /// mauritania': 16.609640474436812,\n",
       " '/// /// torture': 15.609640474436812,\n",
       " '/// /// mali': 16.609640474436812,\n",
       " '/// /// up': 11.655444164049937,\n",
       " '/// /// soldier': 15.024677973715656,\n",
       " '/// /// rwanda': 16.609640474436812,\n",
       " '/// /// relative': 14.609640474436812,\n",
       " '/// /// americas': 16.609640474436812,\n",
       " '/// /// naham': 16.609640474436812,\n",
       " '/// /// anstraum': 16.609640474436812,\n",
       " '/// /// thirty-six': 16.609640474436812,\n",
       " '/// /// viewer': 15.609640474436812,\n",
       " '/// /// brazil': 14.609640474436812,\n",
       " '/// /// ai': 12.702749878828293,\n",
       " '/// /// el': 16.609640474436812,\n",
       " '/// /// military': 13.150208855799514,\n",
       " '/// /// peru': 15.609640474436812,\n",
       " '/// /// dr.': 16.609640474436812,\n",
       " '/// /// asia': 15.609640474436812,\n",
       " '/// /// increase': 13.150208855799514,\n",
       " '/// /// afghanistan': 16.609640474436812,\n",
       " '/// /// sri': 15.024677973715656,\n",
       " '/// /// among': 10.776750460272071,\n",
       " '/// /// taiwan': 16.609640474436812,\n",
       " '/// /// indonesia': 16.609640474436812,\n",
       " '/// /// four': 10.994930630321603,\n",
       " '/// /// europe': 13.024677973715656,\n",
       " '/// /// another': 9.0782590139205,\n",
       " '/// /// turkey': 15.609640474436812,\n",
       " '/// /// suspect': 15.024677973715656,\n",
       " '/// /// police': 11.086078518379798,\n",
       " '/// /// nine': 13.802285552379209,\n",
       " '/// /// ussr': 16.609640474436812,\n",
       " '/// /// figure': 13.150208855799514,\n",
       " '/// /// 15': 15.024677973715656,\n",
       " '/// /// well-known': 15.609640474436812,\n",
       " '/// /// pictur': 14.609640474436812,\n",
       " '/// /// sae': 16.609640474436812,\n",
       " '/// /// middle': 14.609640474436812,\n",
       " '/// /// kuwait': 16.609640474436812,\n",
       " '/// /// report': 13.287712379549449,\n",
       " '/// /// tunisia': 15.024677973715656,\n",
       " '/// /// garde': 16.609640474436812,\n",
       " '/// /// bahrain': 16.609640474436812,\n",
       " '/// /// saudi': 16.609640474436812,\n",
       " '/// /// homenews': 16.609640474436812,\n",
       " '/// /// special': 12.609640474436812,\n",
       " '/// /// joseph': 13.609640474436812,\n",
       " '/// /// keep': 10.937215132465315,\n",
       " '/// /// arnold': 14.609640474436812,\n",
       " '/// /// ma': 14.609640474436812,\n",
       " '/// /// unique': 16.609640474436812,\n",
       " '/// /// free': 12.361712960993227,\n",
       " '/// /// bbc2': 16.609640474436812,\n",
       " '/// /// hong': 11.702749878828293,\n",
       " '/// /// artist': 13.609640474436812,\n",
       " '/// /// collection': 16.609640474436812,\n",
       " '/// /// parcel': 15.024677973715656,\n",
       " '/// /// ?': 14.024677973715656,\n",
       " '/// /// think': 11.655444164049937,\n",
       " '/// /// persuade': 16.609640474436812,\n",
       " '/// /// etc': 14.287712379549449,\n",
       " '/// /// solo': 15.024677973715656,\n",
       " '/// /// though': 9.937215132465315,\n",
       " '/// /// ex-service': 16.609640474436812,\n",
       " '/// /// global': 15.024677973715656,\n",
       " '/// /// moscow': 14.287712379549449,\n",
       " '/// /// sad': 12.4397154729945,\n",
       " '/// /// member': 12.522177633186473,\n",
       " '/// /// such': 9.175012246800087,\n",
       " '/// /// nowaday': 13.609640474436812,\n",
       " '/// /// professional': 14.609640474436812,\n",
       " '/// /// add': 12.522177633186473,\n",
       " '/// /// full-scale': 15.609640474436812,\n",
       " '/// /// opposite': 14.609640474436812,\n",
       " '/// /// campaign': 15.609640474436812,\n",
       " '/// /// freedom': 15.609640474436812,\n",
       " '/// /// year': 13.4397154729945,\n",
       " '/// /// hundred': 12.4397154729945,\n",
       " '/// /// 39': 16.609640474436812,\n",
       " '/// /// hew': 16.609640474436812,\n",
       " '/// /// mulugetta': 16.609640474436812,\n",
       " '/// /// tarcisio': 16.609640474436812,\n",
       " '/// /// isidro': 16.609640474436812,\n",
       " '/// /// luis': 16.609640474436812,\n",
       " '/// /// elizardo': 16.609640474436812,\n",
       " '/// /// agil': 16.609640474436812,\n",
       " '/// /// kayathiri': 16.609640474436812,\n",
       " '/// /// tiao': 16.609640474436812,\n",
       " '/// /// chang': 13.609640474436812,\n",
       " '/// /// father': 12.90920075629572,\n",
       " '/// /// aung': 15.609640474436812,\n",
       " '/// /// naya': 16.609640474436812,\n",
       " '/// /// zhang': 16.609640474436812,\n",
       " '/// /// xiao': 16.609640474436812,\n",
       " '/// /// song': 15.024677973715656,\n",
       " '/// /// ngawang': 16.609640474436812,\n",
       " '/// /// zosimo': 16.609640474436812,\n",
       " '/// /// three': 10.678903136873926,\n",
       " '/// /// ali': 15.024677973715656,\n",
       " '/// /// moham': 14.609640474436812,\n",
       " '/// /// assistant': 15.609640474436812,\n",
       " '/// /// zikri': 16.609640474436812,\n",
       " '/// /// appeal': 15.609640474436812,\n",
       " '/// /// oumarou': 16.609640474436812,\n",
       " '/// /// ismail': 16.609640474436812,\n",
       " '/// /// vincent': 16.609640474436812,\n",
       " '/// /// amos': 16.609640474436812,\n",
       " '/// /// mind': 13.4397154729945,\n",
       " '/// /// human': 14.287712379549449,\n",
       " '/// /// out': 11.609640474436812,\n",
       " '/// /// morocco': 15.609640474436812,\n",
       " '/// /// srifi': 16.609640474436812,\n",
       " '/// /// tell': 12.522177633186473,\n",
       " '/// /// ask': 11.086078518379798,\n",
       " '/// /// malawi': 15.609640474436812,\n",
       " '/// /// write': 13.150208855799514,\n",
       " '/// /// orton': 16.609640474436812,\n",
       " '/// /// philippines': 16.609640474436812,\n",
       " '/// /// maria': 15.609640474436812,\n",
       " '/// /// china': 12.702749878828293,\n",
       " '/// /// xishe': 16.609640474436812,\n",
       " '/// /// israel': 14.287712379549449,\n",
       " '/// /// prisoner': 13.609640474436812,\n",
       " '/// /// each': 9.714822711128868,\n",
       " '/// /// international': 13.609640474436812,\n",
       " '/// /// hamadi': 15.609640474436812,\n",
       " '/// /// fr': 13.287712379549449,\n",
       " '/// /// orlando': 14.609640474436812,\n",
       " '/// /// nijazi': 15.609640474436812,\n",
       " '/// /// kosovo': 16.609640474436812,\n",
       " '/// /// olivier': 16.609640474436812,\n",
       " '/// /// team': 14.609640474436812,\n",
       " '/// /// time': 11.361712960993227,\n",
       " '/// /// dearest': 16.609640474436812,\n",
       " '/// /// rucia': 16.609640474436812,\n",
       " '/// /// president': 12.21732305165805,\n",
       " '/// /// homicides': 16.609640474436812,\n",
       " '/// /// unfortunate': 10.522177633186473,\n",
       " '/// /// capital': 15.609640474436812,\n",
       " '/// /// therefore': 11.183375719734714,\n",
       " '/// /// amazing': 15.024677973715656,\n",
       " '/// /// open': 13.287712379549449,\n",
       " '/// /// juror': 16.609640474436812,\n",
       " '/// /// david': 10.480357457491845,\n",
       " '/// /// along': 13.024677973715656,\n",
       " '/// /// perhaps': 9.009727632249684,\n",
       " '/// /// modell': 15.609640474436812,\n",
       " '/// /// base': 13.150208855799514,\n",
       " '/// /// more': 9.260912320205735,\n",
       " '/// /// predictab': 15.024677973715656,\n",
       " '/// /// repression': 16.609640474436812,\n",
       " '/// /// above': 11.400187108807861,\n",
       " '/// /// left': 13.024677973715656,\n",
       " '/// /// buddhist': 16.609640474436812,\n",
       " '/// /// long': 11.965784284662087,\n",
       " '/// /// turn': 12.4397154729945,\n",
       " '/// /// everyth': 11.565246355078358,\n",
       " '/// /// tazmamert': 16.609640474436812,\n",
       " '/// /// accord': 9.951428991685017,\n",
       " '/// /// noth': 10.90920075629572,\n",
       " '/// /// right': 12.21732305165805,\n",
       " '/// /// political': 13.4397154729945,\n",
       " '/// /// south': 12.702749878828293,\n",
       " '/// /// death': 13.287712379549449,\n",
       " '/// /// detention': 16.609640474436812,\n",
       " '/// /// de': 12.90920075629572,\n",
       " '/// /// accusation': 15.609640474436812,\n",
       " '/// /// refugees': 16.609640474436812,\n",
       " '/// /// kill': 15.609640474436812,\n",
       " '/// /// prior': 14.609640474436812,\n",
       " '/// /// before': 10.361712960993227,\n",
       " '/// /// subsequent': 12.802285552379209,\n",
       " '/// /// tamil': 16.609640474436812,\n",
       " '/// /// 315': 16.609640474436812,\n",
       " '/// /// harold': 14.024677973715656,\n",
       " '/// /// group': 13.802285552379209,\n",
       " '/// /// groups': 16.609640474436812,\n",
       " '/// /// angela': 15.024677973715656,\n",
       " '/// /// participate': 15.609640474436812,\n",
       " '/// /// arthur': 13.287712379549449,\n",
       " '/// /// congratulation': 14.609640474436812,\n",
       " '/// /// jean': 13.4397154729945,\n",
       " '/// /// julia': 13.802285552379209,\n",
       " '/// /// here': 9.296757519152457,\n",
       " '/// /// yet': 8.53282487738598,\n",
       " '/// /// your': 14.287712379549449,\n",
       " '/// /// needles': 13.4397154729945,\n",
       " '/// /// leo': 15.024677973715656,\n",
       " '/// /// art': 11.655444164049937,\n",
       " '/// /// writ': 12.522177633186473,\n",
       " '/// /// article': 13.609640474436812,\n",
       " '/// /// books': 15.024677973715656,\n",
       " '/// /// walter': 13.287712379549449,\n",
       " '/// /// la': 13.150208855799514,\n",
       " '/// /// pater': 15.609640474436812,\n",
       " '/// /// her': 15.024677973715656,\n",
       " '/// /// set': 11.965784284662087,\n",
       " '/// /// certain': 10.480357457491845,\n",
       " '/// /// several': 11.4397154729945,\n",
       " '/// /// let': 10.324238255574564,\n",
       " '/// /// will': 11.117787378107137,\n",
       " '/// /// happi': 13.287712379549449,\n",
       " '/// /// meyer': 16.609640474436812,\n",
       " '/// /// inde': 9.764150423492437,\n",
       " '/// /// fifty': 14.024677973715656,\n",
       " '/// /// venturi': 15.609640474436812,\n",
       " '/// /// german': 14.287712379549449,\n",
       " '/// /// history': 13.287712379549449,\n",
       " '/// /// roger': 12.522177633186473,\n",
       " '/// /// fry': 15.609640474436812,\n",
       " '/// /// critical': 13.609640474436812,\n",
       " '/// /// modern': 12.21732305165805,\n",
       " '/// /// writings': 16.609640474436812,\n",
       " '/// /// teacher': 14.024677973715656,\n",
       " '/// /// book': 12.150208855799514,\n",
       " '/// /// bridget': 15.024677973715656,\n",
       " '/// /// connection': 15.609640474436812,\n",
       " '/// /// james': 11.183375719734714,\n",
       " '/// /// michelangelo': 16.609640474436812,\n",
       " '/// /// critic': 13.4397154729945,\n",
       " '/// /// description': 14.287712379549449,\n",
       " '/// /// interpret': 14.609640474436812,\n",
       " '/// /// amus': 16.609640474436812,\n",
       " '/// /// evocative': 16.609640474436812,\n",
       " '/// /// typical': 12.609640474436812,\n",
       " '/// /// guys': 16.609640474436812,\n",
       " '/// /// modernity': 16.609640474436812,\n",
       " '/// /// baudelaire': 16.609640474436812,\n",
       " '/// /// ruskin': 15.024677973715656,\n",
       " '/// /// parisian': 15.609640474436812,\n",
       " '/// /// surrealist': 16.609640474436812,\n",
       " '/// /// apollinaire': 16.609640474436812,\n",
       " '/// /// gertrude': 16.609640474436812,\n",
       " '/// /// matisse': 15.609640474436812,\n",
       " '/// /// f': 14.024677973715656,\n",
       " '/// /// first': 9.620955787664647,\n",
       " '/// /// read': 13.150208855799514,\n",
       " '/// /// national': 13.024677973715656,\n",
       " '/// /// commentary': 16.609640474436812,\n",
       " '/// /// alfr': 15.609640474436812,\n",
       " '/// /// rather': 11.965784284662087,\n",
       " '/// /// barr': 16.609640474436812,\n",
       " '/// /// joshua': 16.609640474436812,\n",
       " '/// /// raphael': 16.609640474436812,\n",
       " '/// /// reynolds': 14.287712379549449,\n",
       " '/// /// genius': 14.024677973715656,\n",
       " '/// /// literary': 14.024677973715656,\n",
       " '/// /// theory': 14.287712379549449,\n",
       " '/// /// paul': 11.802285552379209,\n",
       " '/// /// sometimes': 10.45989335493213,\n",
       " '/// /// victor': 15.609640474436812,\n",
       " '/// /// theorist': 16.609640474436812,\n",
       " '/// /// post-modernist': 16.609640474436812,\n",
       " '/// /// aerial': 15.609640474436812,\n",
       " '/// /// unlike': 11.252088469818728,\n",
       " '/// /// newer': 16.609640474436812,\n",
       " '/// /// decid': 15.609640474436812,\n",
       " '/// /// jacob': 16.609640474436812,\n",
       " '/// /// vision': 15.024677973715656,\n",
       " '/// /// neither': 11.400187108807861,\n",
       " '/// /// gombrich': 15.609640474436812,\n",
       " '/// /// antoine': 16.609640474436812,\n",
       " '/// /// coysevox': 16.609640474436812,\n",
       " '/// /// excell': 16.609640474436812,\n",
       " '/// /// nevertheless': 10.024677973715656,\n",
       " '/// /// lee': 14.609640474436812,\n",
       " '/// /// attempt': 13.4397154729945,\n",
       " '/// /// w': 13.024677973715656,\n",
       " '/// /// eternal': 16.609640474436812,\n",
       " '/// /// extensive': 14.609640474436812,\n",
       " '/// /// malraux': 16.609640474436812,\n",
       " '/// /// occasional': 12.802285552379209,\n",
       " '/// /// guide': 15.024677973715656,\n",
       " '/// /// old': 11.609640474436812,\n",
       " '/// /// 60c': 16.609640474436812,\n",
       " '/// /// baedeker': 16.609640474436812,\n",
       " '/// /// georgina': 15.609640474436812,\n",
       " '/// /// geographical': 16.609640474436812,\n",
       " '/// /// formal': 15.024677973715656,\n",
       " '/// /// doe': 11.324238255574564,\n",
       " '/// /// consider': 12.150208855799514,\n",
       " '/// /// surveys': 16.609640474436812,\n",
       " '/// /// division': 16.609640474436812,\n",
       " '/// /// limewood': 16.609640474436812,\n",
       " '/// /// difference': 16.609640474436812,\n",
       " '/// /// drawing': 15.609640474436812,\n",
       " '/// /// european': 13.287712379549449,\n",
       " '/// /// watercolour': 16.609640474436812,\n",
       " '/// /// print-mak': 16.609640474436812,\n",
       " '/// /// william': 12.90920075629572,\n",
       " '/// /// kenneth': 13.287712379549449,\n",
       " '/// /// animal': 14.609640474436812,\n",
       " '/// /// exclusion': 16.609640474436812,\n",
       " '/// /// academy': 15.024677973715656,\n",
       " '/// /// later': 10.854752972273344,\n",
       " '/// /// omission': 16.609640474436812,\n",
       " '/// /// post-structuralist': 16.609640474436812,\n",
       " '/// /// zoom': 16.609640474436812,\n",
       " '/// /// subsidy': 15.024677973715656,\n",
       " '/// /// comparison': 14.024677973715656,\n",
       " '/// /// consideration': 15.609640474436812,\n",
       " '/// /// monographs': 15.609640474436812,\n",
       " '/// /// :': 12.802285552379209,\n",
       " '/// /// biographical': 16.609640474436812,\n",
       " '/// /// sainte-beuve': 15.609640474436812,\n",
       " '/// /// proust': 15.609640474436812,\n",
       " '/// /// ironist': 16.609640474436812,\n",
       " '/// /// historical': 13.802285552379209,\n",
       " '/// /// biographies': 16.609640474436812,\n",
       " '/// /// ear': 12.609640474436812,\n",
       " '/// /// lionello': 16.609640474436812,\n",
       " '/// /// leslie': 15.609640474436812,\n",
       " '/// /// constable': 13.4397154729945,\n",
       " '/// /// nordenfalk': 15.609640474436812,\n",
       " '/// /// van': 13.150208855799514,\n",
       " '/// /// lucki': 14.024677973715656,\n",
       " '/// /// private': 12.522177633186473,\n",
       " '/// /// besides': 11.965784284662087,\n",
       " '/// /// lawrence': 15.024677973715656,\n",
       " '/// /// diary': 16.609640474436812,\n",
       " '/// /// battle': 15.024677973715656,\n",
       " '/// /// thomas': 13.4397154729945,\n",
       " '/// /// delacroix': 16.609640474436812,\n",
       " '/// /// rhyme': 16.609640474436812,\n",
       " '/// /// oh': 10.4397154729945,\n",
       " '/// /// sculptural': 15.609640474436812,\n",
       " '/// /// different': 12.702749878828293,\n",
       " '/// /// sculpture': 15.609640474436812,\n",
       " '/// /// twentieth-century': 15.024677973715656,\n",
       " '/// /// commemorative': 16.609640474436812,\n",
       " '/// /// rodin': 16.609640474436812,\n",
       " '/// /// given': 11.21732305165805,\n",
       " '/// /// reproduce': 14.609640474436812,\n",
       " '/// /// further': 11.75165947930924,\n",
       " '/// /// rome': 15.609640474436812,\n",
       " '/// /// terry': 12.802285552379209,\n",
       " '/// /// versatile': 16.609640474436812,\n",
       " '/// /// historian': 15.024677973715656,\n",
       " '/// /// clear': 11.854752972273344,\n",
       " '/// /// pan': 15.609640474436812,\n",
       " '/// /// agassiz': 15.609640474436812,\n",
       " '/// /// morelli': 15.609640474436812,\n",
       " '/// /// berenson': 15.609640474436812,\n",
       " '/// /// max': 15.609640474436812,\n",
       " '/// /// dispute': 16.609640474436812,\n",
       " '/// /// problem': 13.287712379549449,\n",
       " '/// /// catalogues': 16.609640474436812,\n",
       " '/// /// additional': 13.4397154729945,\n",
       " '/// /// exhibition': 15.024677973715656,\n",
       " '/// /// catalogue': 16.609640474436812,\n",
       " '/// /// individual': 13.150208855799514,\n",
       " '/// /// take': 11.117787378107137,\n",
       " '/// /// segment': 16.609640474436812,\n",
       " '/// /// mix': 15.609640474436812,\n",
       " '/// /// general': 11.183375719734714,\n",
       " '/// /// major': 12.702749878828293,\n",
       " '/// /// colvin': 16.609640474436812,\n",
       " '/// /// giacometti': 16.609640474436812,\n",
       " '/// /// kiff': 16.609640474436812,\n",
       " '/// /// mayakovsky': 16.609640474436812,\n",
       " '/// /// museum': 14.609640474436812,\n",
       " '/// /// final': 10.609640474436812,\n",
       " '/// /// auction': 14.609640474436812,\n",
       " '/// /// sothebys': 16.609640474436812,\n",
       " '/// /// carritt': 16.609640474436812,\n",
       " '/// /// part': 11.4397154729945,\n",
       " '/// /// correct': 14.609640474436812,\n",
       " '/// /// theft': 15.609640474436812,\n",
       " '/// /// difficulty': 15.024677973715656,\n",
       " '/// /// happy': 14.024677973715656,\n",
       " '/// /// divid': 15.024677973715656,\n",
       " '/// /// paradoxical': 14.287712379549449,\n",
       " '/// /// old-fashion': 16.609640474436812,\n",
       " '/// /// john': 9.75165947930924,\n",
       " '/// /// johns': 16.609640474436812,\n",
       " '/// /// steinberg': 15.609640474436812,\n",
       " '/// /// incidental': 13.287712379549449,\n",
       " '/// /// reader': 12.609640474436812,\n",
       " '/// /// pop': 15.024677973715656,\n",
       " '/// /// albert': 14.609640474436812,\n",
       " '/// /// barne': 14.287712379549449,\n",
       " '/// /// snapshot': 16.609640474436812,\n",
       " '/// /// except': 13.024677973715656,\n",
       " '/// /// heron': 15.609640474436812,\n",
       " '/// /// personality': 15.609640474436812,\n",
       " '/// /// braque': 16.609640474436812,\n",
       " '/// /// flanner': 16.609640474436812,\n",
       " '/// /// magazine': 15.609640474436812,\n",
       " '/// /// newspaper': 15.024677973715656,\n",
       " '/// /// where': 9.714822711128868,\n",
       " '/// /// unlucki': 16.609640474436812,\n",
       " '/// /// forty-three': 15.609640474436812,\n",
       " '/// /// impressionism': 16.609640474436812,\n",
       " '/// /// seurat': 16.609640474436812,\n",
       " '/// /// denis': 15.609640474436812,\n",
       " '/// /// diderot': 16.609640474436812,\n",
       " '/// /// paint': 13.150208855799514,\n",
       " '/// /// review': 13.802285552379209,\n",
       " '/// /// exhibit': 16.609640474436812,\n",
       " '/// /// themes': 16.609640474436812,\n",
       " '/// /// thematic': 15.609640474436812,\n",
       " '/// /// valuable': 15.609640474436812,\n",
       " '/// /// wolfe': 16.609640474436812,\n",
       " '/// /// greenberg': 16.609640474436812,\n",
       " '/// /// looser': 16.609640474436812,\n",
       " '/// /// choice': 13.802285552379209,\n",
       " '/// /// cultural': 14.287712379549449,\n",
       " '/// /// eduard': 15.609640474436812,\n",
       " '/// /// manz': 16.609640474436812,\n",
       " '/// /// vasari': 16.609640474436812,\n",
       " '/// /// painting': 16.609640474436812,\n",
       " '/// /// frank': 12.702749878828293,\n",
       " '/// /// second': 10.72699742507497,\n",
       " '/// /// chinese': 14.024677973715656,\n",
       " '/// /// perception': 15.609640474436812,\n",
       " '/// /// suppose': 14.287712379549449,\n",
       " '/// /// ways': 16.609640474436812,\n",
       " '/// /// copying': 16.609640474436812,\n",
       " '/// /// copy': 14.024677973715656,\n",
       " '/// /// accurate': 16.609640474436812,\n",
       " '/// /// michael': 10.881720019873612,\n",
       " '/// /// parsons': 16.609640474436812,\n",
       " '/// /// parson': 16.609640474436812,\n",
       " '/// /// herbert': 16.609640474436812,\n",
       " '/// /// picasso': 15.024677973715656,\n",
       " '/// /// whereas': 12.361712960993227,\n",
       " '/// /// question': 13.287712379549449,\n",
       " '/// /// well': 9.554358038935622,\n",
       " '/// /// guerrillas': 15.609640474436812,\n",
       " '/// /// inland': 16.609640474436812,\n",
       " '/// /// nam': 16.609640474436812,\n",
       " '/// /// nathaniel': 16.609640474436812,\n",
       " '/// /// gang': 16.609640474436812,\n",
       " '/// /// tortur': 16.609640474436812,\n",
       " '/// /// everywhere': 13.609640474436812,\n",
       " '/// /// politician': 14.287712379549449,\n",
       " '/// /// ahm': 14.609640474436812,\n",
       " '/// /// enthusiast': 15.609640474436812,\n",
       " '/// /// naipaul': 14.609640474436812,\n",
       " '/// /// jane': 13.287712379549449,\n",
       " '/// /// sodomy': 16.609640474436812,\n",
       " '/// /// together': 13.024677973715656,\n",
       " '/// /// conrad': 15.609640474436812,\n",
       " '/// /// compar': 13.802285552379209,\n",
       " '/// /// race': 11.655444164049937,\n",
       " '/// /// rape': 15.609640474436812,\n",
       " '/// /// hardship': 16.609640474436812,\n",
       " '/// /// salim': 12.90920075629572,\n",
       " '/// /// kenya': 15.609640474436812,\n",
       " '/// /// hitherto': 15.024677973715656,\n",
       " '/// /// present': 13.802285552379209,\n",
       " '/// /// elsewhere': 12.609640474436812,\n",
       " '/// /// metty': 16.609640474436812,\n",
       " '/// /// english': 13.287712379549449,\n",
       " '/// /// virgil': 15.609640474436812,\n",
       " '/// /// subt': 16.609640474436812,\n",
       " '/// /// poor': 12.802285552379209,\n",
       " '/// /// author': 15.609640474436812,\n",
       " '/// /// juliet': 14.609640474436812,\n",
       " '/// /// fraser': 14.024677973715656,\n",
       " '/// /// glasser': 13.802285552379209,\n",
       " '/// /// ronald': 14.024677973715656,\n",
       " '/// /// form': 13.802285552379209,\n",
       " '/// /// bear': 11.702749878828293,\n",
       " '/// /// mrs': 9.923139947253594,\n",
       " '/// /// bert': 15.609640474436812,\n",
       " '/// /// ilse': 16.609640474436812,\n",
       " '/// /// contradiction': 16.609640474436812,\n",
       " '/// /// torn': 16.609640474436812,\n",
       " '/// /// inside': 12.024677973715656,\n",
       " '/// /// understand': 14.609640474436812,\n",
       " '/// /// nail': 15.609640474436812,\n",
       " '/// /// round': 14.609640474436812,\n",
       " '/// /// anti-semitism': 16.609640474436812,\n",
       " '/// /// near': 12.086078518379798,\n",
       " '/// /// plast': 16.609640474436812,\n",
       " '/// /// judg': 14.609640474436812,\n",
       " '/// /// about': 11.086078518379798,\n",
       " '/// /// afterward': 12.21732305165805,\n",
       " '/// /// ralph': 15.609640474436812,\n",
       " '/// /// charlie': 14.609640474436812,\n",
       " '/// /// annie': 14.609640474436812,\n",
       " '/// /// lilian': 15.609640474436812,\n",
       " '/// /// esther': 15.024677973715656,\n",
       " '/// /// julian': 16.609640474436812,\n",
       " '/// /// hawksmoor': 15.609640474436812,\n",
       " '/// /// nicholas': 13.150208855799514,\n",
       " '/// /// dyer': 15.609640474436812,\n",
       " '/// /// much': 10.678903136873926,\n",
       " '/// /// sinclair': 16.609640474436812,\n",
       " '/// /// pastiche': 16.609640474436812,\n",
       " '/// /// ackroyd': 14.287712379549449,\n",
       " '/// /// hora': 16.609640474436812,\n",
       " '/// /// interpreter': 16.609640474436812,\n",
       " '/// /// eliot': 11.90920075629572,\n",
       " '/// /// paranoia': 16.609640474436812,\n",
       " '/// /// romantic': 14.609640474436812,\n",
       " '/// /// charles': 12.287712379549449,\n",
       " '/// /// chatterton': 14.287712379549449,\n",
       " '/// /// influenza': 16.609640474436812,\n",
       " '/// /// constant': 14.609640474436812,\n",
       " '/// /// lecture': 15.024677973715656,\n",
       " '/// /// imposture': 16.609640474436812,\n",
       " '/// /// keat': 15.609640474436812,\n",
       " '/// /// ariel': 16.609640474436812,\n",
       " '/// /// revolution': 16.609640474436812,\n",
       " '/// /// klima': 14.609640474436812,\n",
       " '/// /// head': 13.024677973715656,\n",
       " '/// /// literature': 15.024677973715656,\n",
       " '/// /// tyranny': 15.609640474436812,\n",
       " '/// /// eluard': 16.609640474436812,\n",
       " '/// /// jaromil': 15.024677973715656,\n",
       " '/// /// eugene': 15.609640474436812,\n",
       " '/// /// soon': 11.90920075629572,\n",
       " '/// /// alas': 13.609640474436812,\n",
       " '/// /// magda': 16.609640474436812,\n",
       " '/// /// reality': 15.024677973715656,\n",
       " '/// /// lyricism': 15.024677973715656,\n",
       " '/// /// real': 12.522177633186473,\n",
       " '/// /// kundera': 16.609640474436812,\n",
       " '/// /// youthful': 16.609640474436812,\n",
       " '/// /// poetry': 14.024677973715656,\n",
       " '/// /// mandelstam': 16.609640474436812,\n",
       " '/// /// ivan': 15.609640474436812,\n",
       " '/// /// balloon': 16.609640474436812,\n",
       " '/// /// maybe': 10.854752972273344,\n",
       " '/// /// whichever': 13.802285552379209,\n",
       " '/// /// skvorecky': 16.609640474436812,\n",
       " '/// /// kapuschinski': 16.609640474436812,\n",
       " '/// /// writer': 15.024677973715656,\n",
       " '/// /// kapuscinski': 13.802285552379209,\n",
       " '/// /// face': 12.21732305165805,\n",
       " '/// /// sand': 16.609640474436812,\n",
       " '/// /// foreign': 13.024677973715656,\n",
       " '/// /// bani': 16.609640474436812,\n",
       " '/// /// reject': 14.024677973715656,\n",
       " '/// /// look': 9.965784284662087,\n",
       " '/// /// agostinho': 16.609640474436812,\n",
       " '/// /// crate': 16.609640474436812,\n",
       " '/// /// benguela': 16.609640474436812,\n",
       " '/// /// independ': 16.609640474436812,\n",
       " '/// /// neto': 16.609640474436812,\n",
       " '/// /// heroine': 15.609640474436812,\n",
       " '/// /// t.': 14.024677973715656,\n",
       " '/// /// representative': 14.609640474436812,\n",
       " '/// /// suicide': 14.609640474436812,\n",
       " '/// /// patrician': 16.609640474436812,\n",
       " '/// /// behrens': 16.609640474436812,\n",
       " '/// /// theirs': 13.802285552379209,\n",
       " '/// /// ursula': 14.609640474436812,\n",
       " '/// /// startl': 15.024677973715656,\n",
       " '/// /// chapt': 11.854752972273344,\n",
       " '/// /// kenelm': 16.609640474436812,\n",
       " '/// /// away': 15.609640474436812,\n",
       " '/// /// justin': 15.024677973715656,\n",
       " '/// /// england': 12.361712960993227,\n",
       " '/// /// struggl': 15.024677973715656,\n",
       " '/// /// guesse': 16.609640474436812,\n",
       " '/// /// a.': 14.024677973715656,\n",
       " '/// /// lermontov': 16.609640474436812,\n",
       " '/// /// pechorin': 16.609640474436812,\n",
       " '/// /// blond': 16.609640474436812,\n",
       " '/// /// byron': 15.024677973715656,\n",
       " '/// /// v.': 16.609640474436812,\n",
       " '/// /// kiernan': 16.609640474436812,\n",
       " '/// /// outcome': 16.609640474436812,\n",
       " '/// /// shakespeare': 13.802285552379209,\n",
       " '/// /// wait': 12.702749878828293,\n",
       " '/// /// kingsley': 16.609640474436812,\n",
       " '/// /// graham': 12.702749878828293,\n",
       " '/// /// jenny': 12.90920075629572,\n",
       " '/// /// fair': 14.287712379549449,\n",
       " '/// /// few': 11.21732305165805,\n",
       " '/// /// excellent': 14.024677973715656,\n",
       " '/// /// raconteur': 16.609640474436812,\n",
       " '/// /// amis': 14.287712379549449,\n",
       " '/// /// heir': 16.609640474436812,\n",
       " '/// /// .': 11.287712379549449,\n",
       " '/// /// eric': 14.287712379549449,\n",
       " '/// /// cry': 14.024677973715656,\n",
       " '/// /// mak': 12.702749878828293,\n",
       " '/// /// amid': 14.024677973715656,\n",
       " '/// /// adultery': 16.609640474436812,\n",
       " '/// /// dummy': 16.609640474436812,\n",
       " '/// /// convinc': 15.609640474436812,\n",
       " '/// /// larkin': 14.024677973715656,\n",
       " '/// /// grant': 13.609640474436812,\n",
       " '/// /// true': 12.287712379549449,\n",
       " '/// /// against': 12.4397154729945,\n",
       " '/// /// barbara': 15.609640474436812,\n",
       " '/// /// everett': 16.609640474436812,\n",
       " '/// /// beyond': 12.90920075629572,\n",
       " '/// /// kenner': 16.609640474436812,\n",
       " '/// /// should': 11.854752972273344,\n",
       " '/// /// polymorphou': 16.609640474436812,\n",
       " '/// /// roth': 14.024677973715656,\n",
       " '/// /// defender': 15.609640474436812,\n",
       " '/// /// nathan': 13.4397154729945,\n",
       " '/// /// zuckerman': 14.024677973715656,\n",
       " '/// /// philip': 13.802285552379209,\n",
       " '/// /// shiksa': 16.609640474436812,\n",
       " '/// /// leave': 12.086078518379798,\n",
       " '/// /// portnoy': 16.609640474436812,\n",
       " '/// /// bellow': 15.609640474436812,\n",
       " '/// /// escape': 14.024677973715656,\n",
       " '/// /// marry': 15.609640474436812,\n",
       " '/// /// dualistic': 16.609640474436812,\n",
       " '/// /// brother': 15.609640474436812,\n",
       " '/// /// mail': 15.609640474436812,\n",
       " '/// /// levi': 12.90920075629572,\n",
       " '/// /// herling': 16.609640474436812,\n",
       " '/// /// fascism': 16.609640474436812,\n",
       " '/// /// back': 11.480357457491845,\n",
       " '/// /// seem': 15.609640474436812,\n",
       " '/// /// br': 12.802285552379209,\n",
       " '/// /// babel': 15.609640474436812,\n",
       " '/// /// action': 15.609640474436812,\n",
       " '/// /// faussone': 16.609640474436812,\n",
       " '/// /// book-writer': 16.609640474436812,\n",
       " '/// /// intellig': 15.609640474436812,\n",
       " '/// /// gastric': 16.609640474436812,\n",
       " '/// /// mendel': 16.609640474436812,\n",
       " '/// /// singer': 15.609640474436812,\n",
       " '/// /// s': 13.150208855799514,\n",
       " '/// /// ronnie': 15.609640474436812,\n",
       " '/// /// kelman': 14.609640474436812,\n",
       " '/// /// doyle': 14.287712379549449,\n",
       " '/// /// society': 14.287712379549449,\n",
       " '/// /// shite': 16.609640474436812,\n",
       " '/// /// crassness': 16.609640474436812,\n",
       " '/// /// homework': 16.609640474436812,\n",
       " '/// /// eventual': 11.90920075629572,\n",
       " '/// /// hamlet': 16.609640474436812,\n",
       " '/// /// know': 12.802285552379209,\n",
       " '/// /// gavin': 15.024677973715656,\n",
       " '/// /// show': 13.150208855799514,\n",
       " '/// /// naw': 15.024677973715656,\n",
       " '/// /// good': 11.252088469818728,\n",
       " '/// /// pat': 13.802285552379209,\n",
       " '/// /// throughout': 12.086078518379798,\n",
       " '/// /// object': 16.609640474436812,\n",
       " '/// /// drink': 13.4397154729945,\n",
       " '/// /// stewart': 13.802285552379209,\n",
       " '/// /// less': 11.965784284662087,\n",
       " '/// /// everyone': 11.400187108807861,\n",
       " '/// /// theatre': 12.024677973715656,\n",
       " '/// /// novel': 15.024677973715656,\n",
       " '/// /// academic': 14.287712379549449,\n",
       " '/// /// surprising': 14.609640474436812,\n",
       " '/// /// marlowe': 16.609640474436812,\n",
       " '/// /// sheridan': 16.609640474436812,\n",
       " '/// /// wilde': 16.609640474436812,\n",
       " '/// /// stoppard': 16.609640474436812,\n",
       " '/// /// martin': 12.609640474436812,\n",
       " '/// /// simpson': 16.609640474436812,\n",
       " '/// /// approach': 15.609640474436812,\n",
       " '/// /// notable': 16.609640474436812,\n",
       " '/// /// classe': 14.024677973715656,\n",
       " '/// /// amateur': 15.024677973715656,\n",
       " '/// /// due': 14.287712379549449,\n",
       " '/// /// movement/dance': 16.609640474436812,\n",
       " '/// /// voice/speech': 16.609640474436812,\n",
       " '/// /// lunch': 14.287712379549449,\n",
       " '/// /// make-up': 15.609640474436812,\n",
       " '/// /// rehearsal': 15.609640474436812,\n",
       " '/// /// end': 14.024677973715656,\n",
       " '/// /// anyone': 12.150208855799514,\n",
       " '/// /// go': 10.994930630321603,\n",
       " '/// /// eighty': 15.609640474436812,\n",
       " '/// /// stanislavsky': 15.609640474436812,\n",
       " '/// /// choos': 13.802285552379209,\n",
       " '/// /// two-year': 16.609640474436812,\n",
       " '/// /// lamda': 15.609640474436812,\n",
       " '/// /// drama': 14.287712379549449,\n",
       " '/// /// obvious': 11.802285552379209,\n",
       " '/// /// pay': 13.024677973715656,\n",
       " '/// /// expect': 14.024677973715656,\n",
       " '/// /// three-year': 16.609640474436812,\n",
       " '/// /// applic': 15.609640474436812,\n",
       " '/// /// discretionary': 16.609640474436812,\n",
       " '/// /// scholarship': 16.609640474436812,\n",
       " '/// /// rada': 15.609640474436812,\n",
       " '/// /// become': 15.609640474436812,\n",
       " '/// /// student': 12.287712379549449,\n",
       " '/// /// university': 14.024677973715656,\n",
       " '/// /// undoubted': 14.609640474436812,\n",
       " '/// /// america': 14.287712379549449,\n",
       " '/// /// act': 12.4397154729945,\n",
       " '/// /// remember': 10.587272661408358,\n",
       " '/// /// rigidity': 16.609640474436812,\n",
       " ...}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NG_train_bnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nGram: /// /// i           \t5.008\n",
      "nGram: /// i 've           \t5.36\n",
      "nGram: i 've learnt        \t8.638\n",
      "nGram: 've learnt there    \t20.93\tundef\n",
      "nGram: learnt there be     \t 0.0\n",
      "nGram: there be a          \t2.823\n",
      "nGram: be a lot            \t6.924\n",
      "nGram: a lot of            \t0.6429\n",
      "nGram: lot of car          \t10.3\n",
      "nGram: of car people       \t7.033\n",
      "nGram: car people in       \t20.93\tundef\n",
      "nGram: people in this      \t5.36\n",
      "nGram: in this village     \t10.12\n",
      "nGram: this village ,      \t3.129\n",
      "nGram: village , they      \t8.504\n",
      "nGram: , they 're          \t5.722\n",
      "nGram: they 're keen       \t20.93\tundef\n",
      "nGram: 're keen to         \t 0.0\n",
      "nGram: keen to help        \t5.422\n",
      "nGram: to help in          \t5.469\n",
      "nGram: help in give        \t7.267\n",
      "nGram: in give inform      \t5.728\n",
      "nGram: give inform .       \t4.492\n",
      "PP:171.69\tlen:23\tseg:['i', \"'ve\", 'learnt', 'there', 'be', 'a', 'lot', 'of', 'car', 'people', 'in', 'this', 'village', ',', 'they', \"'re\", 'keen', 'to', 'help', 'in', 'give', 'inform', '.']\n",
      "nGram: /// /// it          \t4.623\n",
      "nGram: /// it '            \t3.709\n",
      "nGram: it ' amaze          \t8.664\n",
      "nGram: ' amaze how         \t 1.7\n",
      "nGram: amaze how we        \t3.858\n",
      "nGram: how we 've          \t20.93\tundef\n",
      "nGram: we 've unit         \t20.93\tundef\n",
      "nGram: 've unit the        \t20.93\tundef\n",
      "nGram: unit the village    \t20.93\tundef\n",
      "nGram: the village and     \t4.271\n",
      "nGram: village and i       \t20.93\tundef\n",
      "nGram: and i feel          \t7.276\n",
      "nGram: i feel good         \t7.389\n",
      "nGram: feel good to        \t4.833\n",
      "nGram: good to be          \t2.555\n",
      "nGram: to be a             \t3.982\n",
      "nGram: be a part           \t8.422\n",
      "nGram: a part of           \t0.713\n",
      "nGram: part of this        \t5.63\n",
      "nGram: of this village     \t11.16\n",
      "PP:576.96\tlen:20\tseg:['it', \"'\", 'amaze', 'how', 'we', \"'ve\", 'unit', 'the', 'village', 'and', 'i', 'feel', 'good', 'to', 'be', 'a', 'part', 'of', 'this', 'village']\n",
      "nGram: /// /// paul        \t11.75\n",
      "nGram: /// paul copperwheat\t20.93\tundef\n",
      "PP:82902.67\tlen:2\tseg:['paul', 'copperwheat']\n",
      "Number of OOV 3-grams: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[171.6871757926335, 576.9638281114514, 82902.6672289678]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(apply_kent_stemmer([word_tokenize(l) for l in bnc[5000000:5000003]]), NG_train_bnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nGram: /// i               \t5.008\n",
      "nGram: i 've               \t5.749\n",
      "nGram: 've learnt          \t9.007\n",
      "nGram: learnt there        \t8.951\n",
      "nGram: there be            \t1.324\n",
      "nGram: be a                \t4.35\n",
      "nGram: a lot               \t7.194\n",
      "nGram: lot of              \t0.703\n",
      "nGram: of car              \t11.2\n",
      "nGram: car people          \t10.58\n",
      "nGram: people in           \t4.306\n",
      "nGram: in this             \t5.849\n",
      "nGram: this village        \t11.24\n",
      "nGram: village ,           \t3.019\n",
      "nGram: , they              \t6.822\n",
      "nGram: they 're            \t6.818\n",
      "nGram: 're keen            \t12.42\n",
      "nGram: keen to             \t1.411\n",
      "nGram: to help             \t7.848\n",
      "nGram: help in             \t5.091\n",
      "nGram: in give             \t12.83\n",
      "nGram: give inform         \t8.071\n",
      "nGram: inform .            \t3.81\n",
      "PP:102.38\tlen:23\tseg:['i', \"'ve\", 'learnt', 'there', 'be', 'a', 'lot', 'of', 'car', 'people', 'in', 'this', 'village', ',', 'they', \"'re\", 'keen', 'to', 'help', 'in', 'give', 'inform', '.']\n",
      "nGram: /// it              \t4.623\n",
      "nGram: it '                \t4.412\n",
      "nGram: ' amaze             \t10.37\n",
      "nGram: amaze how           \t3.949\n",
      "nGram: how we              \t6.112\n",
      "nGram: we 've              \t6.503\n",
      "nGram: 've unit            \t21.21\tundef\n",
      "nGram: unit the            \t7.519\n",
      "nGram: the village         \t9.989\n",
      "nGram: village and         \t4.467\n",
      "nGram: and i               \t6.222\n",
      "nGram: i feel              \t7.483\n",
      "nGram: feel good           \t7.123\n",
      "nGram: good to             \t5.832\n",
      "nGram: to be               \t3.745\n",
      "nGram: be a                \t4.35\n",
      "nGram: a part              \t9.534\n",
      "nGram: part of             \t0.7734\n",
      "nGram: of this             \t6.479\n",
      "nGram: this village        \t11.24\n",
      "PP:136.90\tlen:20\tseg:['it', \"'\", 'amaze', 'how', 'we', \"'ve\", 'unit', 'the', 'village', 'and', 'i', 'feel', 'good', 'to', 'be', 'a', 'part', 'of', 'this', 'village']\n",
      "nGram: /// paul            \t11.75\n",
      "nGram: paul copperwheat    \t21.21\tundef\n",
      "PP:91436.94\tlen:2\tseg:['paul', 'copperwheat']\n",
      "Number of OOV 2-grams: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[102.38111844077585, 136.89898237494077, 91436.94426561019]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity2(apply_kent_stemmer([word_tokenize(l) for l in bnc[5000000:5000003]]), NG_train_bnc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nGram: at                  \t7.761\n",
      "nGram: oth                 \t9.724\n",
      "nGram: time                \t9.43\n",
      "nGram: ,                   \t4.467\n",
      "nGram: it                  \t6.746\n",
      "nGram: have                \t6.66\n",
      "nGram: pursu               \t15.26\n",
      "nGram: a                   \t5.677\n",
      "nGram: policy              \t11.79\n",
      "nGram: of                  \t5.247\n",
      "nGram: let                 \t11.91\n",
      "nGram: the                 \t4.215\n",
      "nGram: pound               \t9.495\n",
      "nGram: float               \t15.35\n",
      "PP:304.19\tlen:15\tseg:['at', 'oth', 'time', ',', 'it', 'have', 'pursu', 'a', 'policy', 'of', 'let', 'the', 'pound', 'float', '.']\n",
      "nGram: these               \t10.18\n",
      "nGram: policy              \t11.79\n",
      "nGram: be                  \t5.552\n",
      "nGram: it                  \t6.746\n",
      "nGram: own                 \t10.39\n",
      "PP:174.10\tlen:6\tseg:['these', 'policy', 'be', 'it', 'own', '.']\n",
      "nGram: in                  \t5.904\n",
      "nGram: any                 \t9.953\n",
      "nGram: case                \t11.08\n",
      "nGram: ,                   \t4.467\n",
      "nGram: the                 \t4.215\n",
      "nGram: depend              \t13.34\n",
      "nGram: on                  \t7.087\n",
      "nGram: germany             \t12.81\n",
      "nGram: is                  \t6.607\n",
      "nGram: exaggerat           \t17.23\n",
      "nGram: :                   \t8.493\n",
      "nGram: during              \t11.42\n",
      "nGram: 1991                \t13.19\n",
      "nGram: british             \t10.99\n",
      "nGram: interest            \t10.97\n",
      "nGram: rate                \t11.41\n",
      "nGram: have                \t6.66\n",
      "nGram: be                  \t5.552\n",
      "nGram: fall                \t12.4\n",
      "nGram: ,                   \t4.467\n",
      "nGram: while               \t10.92\n",
      "nGram: german              \t12.45\n",
      "nGram: rate                \t11.41\n",
      "nGram: have                \t6.66\n",
      "nGram: be                  \t5.552\n",
      "nGram: ris                 \t14.34\n",
      "PP:606.37\tlen:27\tseg:['in', 'any', 'case', ',', 'the', 'depend', 'on', 'germany', 'is', 'exaggerat', ':', 'during', '1991', 'british', 'interest', 'rate', 'have', 'be', 'fall', ',', 'while', 'german', 'rate', 'have', 'be', 'ris', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[304.1909083832307, 174.1027351441073, 606.3685342526851]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity1(apply_kent_stemmer([word_tokenize(l) for l in bnc_sent[500000:500003]]), NG_train_bnc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
