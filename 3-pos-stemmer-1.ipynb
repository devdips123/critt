{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos-conditioned KENTstemmer\n",
    "1. import text\n",
    "2. PoS-tag text\n",
    "3. lemmatize text\n",
    "4. evaluate lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk resources for lemmatizer evaluation\n",
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag(nltk.book.text1)\n",
    "tag[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KENTstemmer3: read a tuple (word, PoS) return stem\n",
    "def KENTstemmer3(t, dic) :  \n",
    "        \n",
    "    (wrd, pos) = t\n",
    "    stem = wrd.lower()\n",
    "    if(stem in dic) : stem = dic[stem]\n",
    "                                 \n",
    "    if(pos == \"VBD\"):\n",
    "        stem = re.sub(r'(ed)$', '', stem)\n",
    "\n",
    "    elif(pos == \"VBN\"):\n",
    "        stem = re.sub(r'(ed)$', '', stem)\n",
    "        \n",
    "    elif(pos == \"VBG\"):\n",
    "        stem = re.sub(r'(ing)$', '', stem)             \n",
    "        \n",
    "    elif(pos == \"VBZ\"):\n",
    "        stem = re.sub(r'(s)$', '', stem)\n",
    "     \n",
    "    elif(pos == \"NN\"):\n",
    "        stem = re.sub(r'(ment|ence|ation)$', '', stem)\n",
    "\n",
    "    elif(pos == \"NNS\"):\n",
    "        stem = re.sub(r'(s|ies)$', '', stem)\n",
    "        \n",
    "    elif(pos == \"NNP\"):\n",
    "        stem = re.sub(r'(ed)$', '', stem)\n",
    "        \n",
    "    elif(pos ==\"JJ\"):\n",
    "        stem = re.sub(r'(est|ing|ly)$', '', stem)\n",
    "\n",
    "    elif(pos == \"JJR\"):\n",
    "        stem = re.sub(r'(er)$', '', stem)               \n",
    "        \n",
    "    elif(pos == \"JJS\"):\n",
    "        stem = re.sub(r'(est)$', '', stem)\n",
    "        \n",
    "    elif(pos == \"RBR\"):\n",
    "        stem = re.sub(r'(er)$', '', stem)               \n",
    "        \n",
    "    elif(pos == \"RBS\"):\n",
    "        stem = re.sub(r'(est)$', '', stem)                \n",
    "        \n",
    "    elif(pos == \"RB\"):\n",
    "        stem = re.sub(r'(ly|ed)$', '', stem)    \n",
    "        \n",
    "    elif(pos == \"FW\"):\n",
    "        stem = re.sub(r'(o)$', '', stem)                \n",
    "    \n",
    "    return(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a exception dictionary\n",
    "\n",
    "def readDictionary(dictionary) :\n",
    "    Dic = {}\n",
    "    with open(dictionary,\"r\", encoding=\"utf8\") as file:\n",
    "        for entry in file:\n",
    "            lem, tok = re.findall(\"^(.*?)[\\s]+(.*?)$\", entry)[0]\n",
    "            Dic[tok.lower()] = lem.lower()\n",
    "    return (Dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download additional packages if required\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Import porter and snowball wordnet stemmers\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# instantiate stemmers \n",
    "wordNet = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "\n",
    "# read reference lemmas (adjust the path)\n",
    "BNClemma = readDictionary(\"lemmaLexicon/BNC_lemmafile5-unix.txt\")\n",
    "\n",
    "# tokenize NLTK text1 \n",
    "tok = nltk.book.text1\n",
    "\n",
    "# pos-tag text for the KENTstemmer\n",
    "tag = nltk.pos_tag(tok)\n",
    "\n",
    "# number of matching lemmas\n",
    "kentM = portM = snowM = wnM = bncM = 0\n",
    "\n",
    "# read exception lexicon for KENTstemmer\n",
    "dic = readDictionary(\"lemmaLexicon/tokenLemma.txt\")\n",
    "\n",
    "# count correctly stemmed words, which are in the BNClemma lexicon\n",
    "for w, p in tag :\n",
    "    if w in BNClemma:\n",
    "        # count matching lemmas\n",
    "        if(BNClemma[w] == KENTstemmer3((w, p), dic)) : kentM = kentM + 1\n",
    "        if(BNClemma[w] == porter.stem(w)) : portM = portM + 1\n",
    "        if(BNClemma[w] == snowball.stem(w)) : snowM = snowM + 1\n",
    "        if(BNClemma[w] == wordNet.lemmatize(w)) : wnM = wnM + 1\n",
    "        bncM = bncM + 1\n",
    "\n",
    "# print out number of matched tokens\n",
    "print(\"Tokens:{} Match:{} KENT:{} PORT:{} SNOW:{} WN:{}\".format(len(tag), bncM, kentM, portM, snowM, wnM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store lists of produced lemmas\n",
    "kentL = portL = snowL = wnL = bncL = word = pos = []\n",
    "\n",
    "kentL = [KENTstemmer3((w, p), dic)  for w, p in tag if w in BNClemma]\n",
    "portL = [porter.stem(w)             for w, p in tag if w in BNClemma]\n",
    "snowL = [snowball.stem(w)           for w, p in tag if w in BNClemma]\n",
    "wnL   = [wordNet.lemmatize(w)       for w, p in tag if w in BNClemma]\n",
    "bncL  = [BNClemma[w]                for w, p in tag if w in BNClemma]\n",
    "word  = [w                          for w, p in tag if w in BNClemma]\n",
    "pos   = [p                          for w, p in tag if w in BNClemma]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks: write a function to measure accuracy\n",
    "#  measure accuracy of the four stemmers\n",
    "\n",
    "def accuracy(l1, l2):\n",
    "# ....\n",
    "    return #the accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "print(\"accuray KENT:{:4.4} PORT:{:4.4} SNOW:{:4.4} WN:{:4.4}\".\n",
    "      format(accuracy(kentL, bncL), \n",
    "             accuracy(portL, bncL),\n",
    "             accuracy(snowL, bncL),\n",
    "             accuracy(wnL, bncL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tasks: \n",
    "#  add better lemmatization rules \n",
    "#  use different text material (nltk.book.text1, nltk.book.text2, nltk.book.text3, ...)\n",
    "#  compute type / token ratio for each lemmatizer\n",
    "#  compare with accuracy of KENTstemmer0\n",
    "#  check frequency distribution of lemmatized / non-lemmatized words\n",
    "#  check which words are not lemmatized and why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# delete duplicates from the list of words / lemmas / stems\n",
    "df = pd.DataFrame({'token': word, 'BNC' : bncL, 'kent3' : kentL, 'porter': portL, 'snow':snowL, 'WN' : wnL, 'pos':pos})\n",
    "\n",
    "# delete duplicates from the list of words / lemmas / stems\n",
    "df1 = df[[\"token\", \"BNC\", \"kent3\", \"porter\", \"snow\", \"WN\", \"pos\"]].drop_duplicates()\n",
    "\n",
    "# delete all rows in which the corrwct lemmatization\n",
    "df2 = [df1.iloc[[i]] for i in range(0,df1.shape[0]) if (df1.iloc[i][\"kent3\"] != df1.iloc[i][\"BNC\"])]\n",
    "\n",
    "# sort the list by PoS and print out\n",
    "result = pd.concat(df2).sort_values(by=\"pos\")\n",
    "result.head(50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
